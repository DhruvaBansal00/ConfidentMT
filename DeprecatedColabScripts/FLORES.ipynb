{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FLORES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvaBansal00/ConfidentMT/blob/master/FLORES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh-FGunpdnZ-",
        "colab_type": "code",
        "outputId": "997fc7c0-736b-4c98-bbdf-43e2ee3aa927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "pip install fairseq sacrebleu sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 2.8MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/58/5c6cc352ea6271125325950715cf8b59b77abe5e93cf29f6e60b491a31d9/sacrebleu-1.4.6-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.38.0)\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 172kB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2015441 sha256=e3ca8d821f9f635880997ab8591ef6da9a1333a7e3bdd738a46e9b5fac7726cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: mecab-python3, portalocker, sacrebleu, fairseq, sentencepiece\n",
            "Successfully installed fairseq-0.9.0 mecab-python3-0.996.5 portalocker-1.7.0 sacrebleu-1.4.6 sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMDt_Ek5dk95",
        "colab_type": "code",
        "outputId": "21f984c5-7b07-4515-f1a2-d2d504f7047f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwXH197JdiWE",
        "colab_type": "code",
        "outputId": "bb44a3e3-23ba-4950-bc81-c6a45954fe9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!ls\n",
        "%cd drive/My Drive/ConfidentMachineTranslation/flores\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "/content/drive/My Drive/ConfidentMachineTranslation/flores\n",
            " Analysis\t\t\t        FAIRPaper.pdf\n",
            " checkpoints\t\t\t        FLORES.ipynb\n",
            "'Confidence Low Res MT Progress.gdoc'   prepare-monolingual.sh\n",
            "'Confident MT.PDF'\t\t        prepare-neen.sh\n",
            " configs\t\t\t        prepare-sien.sh\n",
            " data\t\t\t\t        README.md\n",
            " data-bin\t\t\t        removed\n",
            " download-data.sh\t\t        reproduce.sh\n",
            " Ensembles\t\t\t        scripts\n",
            " Ensembling\t\t\t        sentence_level_bleu\n",
            "/content/drive/My Drive/ConfidentMachineTranslation/flores\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFUFP7kEqqjE",
        "colab_type": "text"
      },
      "source": [
        "Train ne-en model using this command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKLjvoNdd4E",
        "colab_type": "code",
        "outputId": "f7beaf2d-3637-4fad-ae3f-862e46937600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
        "    data-bin/wiki_ne_en_bpe5000/ \\\n",
        "    --source-lang ne --target-lang en \\\n",
        "    --arch transformer --share-all-embeddings \\\n",
        "    --encoder-layers 5 --decoder-layers 5 \\\n",
        "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
        "    --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
        "    --encoder-attention-heads 2 --decoder-attention-heads 2 \\\n",
        "    --encoder-normalize-before --decoder-normalize-before \\\n",
        "    --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --label-smoothing 0.2 --criterion label_smoothed_cross_entropy \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \\\n",
        "    --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \\\n",
        "    --lr 1e-3 --min-lr 1e-9 \\\n",
        "    --max-tokens 4000 \\\n",
        "    --update-freq 4 \\\n",
        "    --max-epoch 100 --save-interval 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation_dropout=0.2, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.2, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wiki_ne_en_bpe5000/', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=2, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=5, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.4, empty_cache_freq=0, encoder_attention_heads=2, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=5, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.2, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=10, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='ne', target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)\n",
            "| [ne] dictionary: 5000 types\n",
            "| [en] dictionary: 5000 types\n",
            "| loaded 2559 examples from: data-bin/wiki_ne_en_bpe5000/valid.ne-en.ne\n",
            "| loaded 2559 examples from: data-bin/wiki_ne_en_bpe5000/valid.ne-en.en\n",
            "| data-bin/wiki_ne_en_bpe5000/ valid ne-en 2559 examples\n",
            "TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(5000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(5000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "| model transformer, criterion LabelSmoothedCrossEntropyCriterion\n",
            "| num. model params: 39344128 (num. trained: 39344128)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 4000 and max sentences per GPU = None\n",
            "| loaded checkpoint checkpoints/checkpoint_last.pt (epoch 20 @ 11400 updates)\n",
            "| loading train data for epoch 20\n",
            "| loaded 563779 examples from: data-bin/wiki_ne_en_bpe5000/train.ne-en.ne\n",
            "| loaded 563779 examples from: data-bin/wiki_ne_en_bpe5000/train.ne-en.en\n",
            "| data-bin/wiki_ne_en_bpe5000/ train ne-en 563779 examples\n",
            "| epoch 021 | loss 5.027 | nll_loss 2.522 | ppl 5.74 | wps 17324 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 11970 | lr 0.000578073 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 437 | train_wall 8625\n",
            "| epoch 021 | valid on 'valid' subset | loss 6.942 | nll_loss 4.735 | ppl 26.63 | num_updates 11970 | best_loss 6.94176\n",
            "| epoch 022 | loss 5.005 | nll_loss 2.494 | ppl 5.63 | wps 17265 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 12540 | lr 0.000564782 | gnorm 0.228 | clip 0.000 | oom 0.000 | wall 868 | train_wall 9022\n",
            "| epoch 022 | valid on 'valid' subset | loss 6.924 | nll_loss 4.691 | ppl 25.83 | num_updates 12540 | best_loss 6.92352\n",
            "| epoch 023 | loss 4.989 | nll_loss 2.474 | ppl 5.55 | wps 17317 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 13110 | lr 0.000552368 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 1298 | train_wall 9418\n",
            "| epoch 023 | valid on 'valid' subset | loss 6.937 | nll_loss 4.715 | ppl 26.27 | num_updates 13110 | best_loss 6.93676\n",
            "| epoch 024 | loss 4.971 | nll_loss 2.451 | ppl 5.47 | wps 17485 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 13680 | lr 0.000540738 | gnorm 0.227 | clip 0.000 | oom 0.000 | wall 1723 | train_wall 9811\n",
            "| epoch 024 | valid on 'valid' subset | loss 6.924 | nll_loss 4.708 | ppl 26.13 | num_updates 13680 | best_loss 6.92447\n",
            "| epoch 025 | loss 4.954 | nll_loss 2.431 | ppl 5.39 | wps 17463 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 14250 | lr 0.000529813 | gnorm 0.226 | clip 0.000 | oom 0.000 | wall 2149 | train_wall 10204\n",
            "| epoch 025 | valid on 'valid' subset | loss 6.919 | nll_loss 4.692 | ppl 25.84 | num_updates 14250 | best_loss 6.91915\n",
            "| epoch 026 | loss 4.940 | nll_loss 2.412 | ppl 5.32 | wps 17446 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 14820 | lr 0.000519524 | gnorm 0.225 | clip 0.000 | oom 0.000 | wall 2575 | train_wall 10598\n",
            "| epoch 026 | valid on 'valid' subset | loss 6.909 | nll_loss 4.674 | ppl 25.53 | num_updates 14820 | best_loss 6.90859\n",
            "| epoch 027 | loss 4.925 | nll_loss 2.394 | ppl 5.26 | wps 17454 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 15390 | lr 0.000509813 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 3002 | train_wall 10991\n",
            "| epoch 027 | valid on 'valid' subset | loss 6.905 | nll_loss 4.670 | ppl 25.46 | num_updates 15390 | best_loss 6.90453\n",
            "| epoch 028 | loss 4.913 | nll_loss 2.378 | ppl 5.2 | wps 17437 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 15960 | lr 0.000500626 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 3428 | train_wall 11384\n",
            "| epoch 028 | valid on 'valid' subset | loss 6.883 | nll_loss 4.648 | ppl 25.07 | num_updates 15960 | best_loss 6.8826\n",
            "| epoch 029 | loss 4.900 | nll_loss 2.361 | ppl 5.14 | wps 17466 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 16530 | lr 0.000491919 | gnorm 0.222 | clip 0.000 | oom 0.000 | wall 3854 | train_wall 11777\n",
            "| epoch 029 | valid on 'valid' subset | loss 6.861 | nll_loss 4.630 | ppl 24.76 | num_updates 16530 | best_loss 6.86145\n",
            "| epoch 030 | loss 4.887 | nll_loss 2.346 | ppl 5.09 | wps 17382 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 17100 | lr 0.000483651 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 4282 | train_wall 12172\n",
            "| epoch 030 | valid on 'valid' subset | loss 6.875 | nll_loss 4.641 | ppl 24.94 | num_updates 17100 | best_loss 6.8753\n",
            "| saved checkpoint checkpoints/checkpoint30.pt (epoch 30 @ 17100 updates) (writing took 38.4409122467041 seconds)\n",
            "| epoch 031 | loss 4.877 | nll_loss 2.334 | ppl 5.04 | wps 17343 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 17670 | lr 0.000475786 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 4749 | train_wall 12566\n",
            "| epoch 031 | valid on 'valid' subset | loss 6.832 | nll_loss 4.588 | ppl 24.05 | num_updates 17670 | best_loss 6.83186\n",
            "| epoch 032 | loss 4.866 | nll_loss 2.320 | ppl 4.99 | wps 17380 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 18240 | lr 0.000468293 | gnorm 0.222 | clip 0.000 | oom 0.000 | wall 5178 | train_wall 12961\n",
            "| epoch 032 | valid on 'valid' subset | loss 6.841 | nll_loss 4.601 | ppl 24.27 | num_updates 18240 | best_loss 6.84074\n",
            "| epoch 033 | loss 4.856 | nll_loss 2.307 | ppl 4.95 | wps 17386 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 18810 | lr 0.000461143 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 5605 | train_wall 13355\n",
            "| epoch 033 | valid on 'valid' subset | loss 6.847 | nll_loss 4.616 | ppl 24.52 | num_updates 18810 | best_loss 6.84727\n",
            "| epoch 034 | loss 4.847 | nll_loss 2.296 | ppl 4.91 | wps 17313 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 19380 | lr 0.000454311 | gnorm 0.222 | clip 0.000 | oom 0.000 | wall 6035 | train_wall 13750\n",
            "| epoch 034 | valid on 'valid' subset | loss 6.838 | nll_loss 4.584 | ppl 23.99 | num_updates 19380 | best_loss 6.83757\n",
            "| epoch 035 | loss 4.837 | nll_loss 2.283 | ppl 4.87 | wps 17352 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 19950 | lr 0.000447774 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 6464 | train_wall 14145\n",
            "| epoch 035 | valid on 'valid' subset | loss 6.839 | nll_loss 4.587 | ppl 24.04 | num_updates 19950 | best_loss 6.83909\n",
            "| epoch 036 | loss 4.829 | nll_loss 2.273 | ppl 4.83 | wps 17378 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 20520 | lr 0.000441511 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 6892 | train_wall 14540\n",
            "| epoch 036 | valid on 'valid' subset | loss 6.805 | nll_loss 4.560 | ppl 23.6 | num_updates 20520 | best_loss 6.80539\n",
            "| epoch 037 | loss 4.821 | nll_loss 2.263 | ppl 4.8 | wps 17407 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 21090 | lr 0.000435504 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 7319 | train_wall 14934\n",
            "| epoch 037 | valid on 'valid' subset | loss 6.844 | nll_loss 4.607 | ppl 24.37 | num_updates 21090 | best_loss 6.84402\n",
            "| epoch 038 | loss 4.813 | nll_loss 2.253 | ppl 4.77 | wps 17339 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 21660 | lr 0.000429735 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 7748 | train_wall 15329\n",
            "| epoch 038 | valid on 'valid' subset | loss 6.771 | nll_loss 4.521 | ppl 22.95 | num_updates 21660 | best_loss 6.771\n",
            "| epoch 039 | loss 4.806 | nll_loss 2.244 | ppl 4.74 | wps 17284 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 22230 | lr 0.00042419 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 8179 | train_wall 15725\n",
            "| epoch 039 | valid on 'valid' subset | loss 6.796 | nll_loss 4.542 | ppl 23.3 | num_updates 22230 | best_loss 6.7958\n",
            "| epoch 040 | loss 4.797 | nll_loss 2.234 | ppl 4.7 | wps 17430 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 22800 | lr 0.000418854 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 8606 | train_wall 16119\n",
            "| epoch 040 | valid on 'valid' subset | loss 6.831 | nll_loss 4.582 | ppl 23.95 | num_updates 22800 | best_loss 6.8307\n",
            "| saved checkpoint checkpoints/checkpoint40.pt (epoch 40 @ 22800 updates) (writing took 6.4480438232421875 seconds)\n",
            "| epoch 041 | loss 4.790 | nll_loss 2.224 | ppl 4.67 | wps 17387 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 23370 | lr 0.000413714 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 9040 | train_wall 16513\n",
            "| epoch 041 | valid on 'valid' subset | loss 6.796 | nll_loss 4.543 | ppl 23.31 | num_updates 23370 | best_loss 6.79592\n",
            "| epoch 042 | loss 4.783 | nll_loss 2.216 | ppl 4.65 | wps 17361 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 23940 | lr 0.00040876 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 9468 | train_wall 16908\n",
            "| epoch 042 | valid on 'valid' subset | loss 6.787 | nll_loss 4.533 | ppl 23.15 | num_updates 23940 | best_loss 6.78747\n",
            "| epoch 043 | loss 4.777 | nll_loss 2.208 | ppl 4.62 | wps 17338 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 24510 | lr 0.000403979 | gnorm 0.218 | clip 0.000 | oom 0.000 | wall 9898 | train_wall 17302\n",
            "| epoch 043 | valid on 'valid' subset | loss 6.811 | nll_loss 4.558 | ppl 23.56 | num_updates 24510 | best_loss 6.81083\n",
            "| epoch 044 | loss 4.771 | nll_loss 2.200 | ppl 4.6 | wps 17291 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 25080 | lr 0.000399362 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 10328 | train_wall 17698\n",
            "| epoch 044 | valid on 'valid' subset | loss 6.795 | nll_loss 4.548 | ppl 23.39 | num_updates 25080 | best_loss 6.79489\n",
            "| epoch 045 | loss 4.764 | nll_loss 2.192 | ppl 4.57 | wps 17404 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 25650 | lr 0.000394899 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 10755 | train_wall 18092\n",
            "| epoch 045 | valid on 'valid' subset | loss 6.790 | nll_loss 4.539 | ppl 23.25 | num_updates 25650 | best_loss 6.79017\n",
            "| epoch 046 | loss 4.758 | nll_loss 2.185 | ppl 4.55 | wps 17459 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 26220 | lr 0.000390583 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 11181 | train_wall 18486\n",
            "| epoch 046 | valid on 'valid' subset | loss 6.781 | nll_loss 4.522 | ppl 22.98 | num_updates 26220 | best_loss 6.78055\n",
            "| epoch 047 | loss 4.752 | nll_loss 2.177 | ppl 4.52 | wps 17483 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 26790 | lr 0.000386406 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 11607 | train_wall 18879\n",
            "| epoch 047 | valid on 'valid' subset | loss 6.761 | nll_loss 4.497 | ppl 22.58 | num_updates 26790 | best_loss 6.76127\n",
            "| epoch 048 | loss 4.747 | nll_loss 2.171 | ppl 4.5 | wps 17451 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 27360 | lr 0.00038236 | gnorm 0.218 | clip 0.000 | oom 0.000 | wall 12033 | train_wall 19272\n",
            "| epoch 048 | valid on 'valid' subset | loss 6.779 | nll_loss 4.521 | ppl 22.96 | num_updates 27360 | best_loss 6.77857\n",
            "| epoch 049 | loss 4.742 | nll_loss 2.164 | ppl 4.48 | wps 17492 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 27930 | lr 0.000378438 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 12458 | train_wall 19665\n",
            "| epoch 049 | valid on 'valid' subset | loss 6.740 | nll_loss 4.487 | ppl 22.43 | num_updates 27930 | best_loss 6.73973\n",
            "| epoch 050 | loss 4.737 | nll_loss 2.159 | ppl 4.47 | wps 17497 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 28500 | lr 0.000374634 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 12884 | train_wall 20058\n",
            "| epoch 050 | valid on 'valid' subset | loss 6.795 | nll_loss 4.539 | ppl 23.26 | num_updates 28500 | best_loss 6.79526\n",
            "| saved checkpoint checkpoints/checkpoint50.pt (epoch 50 @ 28500 updates) (writing took 6.8486433029174805 seconds)\n",
            "| epoch 051 | loss 4.732 | nll_loss 2.152 | ppl 4.44 | wps 17483 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 29070 | lr 0.000370943 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 13316 | train_wall 20451\n",
            "| epoch 051 | valid on 'valid' subset | loss 6.793 | nll_loss 4.540 | ppl 23.27 | num_updates 29070 | best_loss 6.79256\n",
            "| epoch 052 | loss 4.726 | nll_loss 2.145 | ppl 4.42 | wps 17517 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 29640 | lr 0.000367359 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 13741 | train_wall 20844\n",
            "| epoch 052 | valid on 'valid' subset | loss 6.745 | nll_loss 4.488 | ppl 22.43 | num_updates 29640 | best_loss 6.7448\n",
            "| epoch 053 | loss 4.722 | nll_loss 2.139 | ppl 4.41 | wps 17519 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 30210 | lr 0.000363877 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 14165 | train_wall 21237\n",
            "| epoch 053 | valid on 'valid' subset | loss 6.768 | nll_loss 4.522 | ppl 22.97 | num_updates 30210 | best_loss 6.76799\n",
            "| epoch 054 | loss 4.718 | nll_loss 2.134 | ppl 4.39 | wps 17506 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 30780 | lr 0.000360492 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 14590 | train_wall 21630\n",
            "| epoch 054 | valid on 'valid' subset | loss 6.779 | nll_loss 4.531 | ppl 23.12 | num_updates 30780 | best_loss 6.77869\n",
            "| epoch 055 | loss 4.712 | nll_loss 2.127 | ppl 4.37 | wps 17496 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 31350 | lr 0.0003572 | gnorm 0.218 | clip 0.000 | oom 0.000 | wall 15016 | train_wall 22022\n",
            "| epoch 055 | valid on 'valid' subset | loss 6.783 | nll_loss 4.526 | ppl 23.04 | num_updates 31350 | best_loss 6.78302\n",
            "| epoch 056 | loss 4.709 | nll_loss 2.123 | ppl 4.36 | wps 17513 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 31920 | lr 0.000353996 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 15440 | train_wall 22415\n",
            "| epoch 056 | valid on 'valid' subset | loss 6.726 | nll_loss 4.461 | ppl 22.02 | num_updates 31920 | best_loss 6.72602\n",
            "| epoch 057 | loss 4.704 | nll_loss 2.117 | ppl 4.34 | wps 17486 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 32490 | lr 0.000350877 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 15866 | train_wall 22808\n",
            "| epoch 057 | valid on 'valid' subset | loss 6.724 | nll_loss 4.463 | ppl 22.05 | num_updates 32490 | best_loss 6.72396\n",
            "| epoch 058 | loss 4.700 | nll_loss 2.112 | ppl 4.32 | wps 17497 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 33060 | lr 0.000347839 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 16291 | train_wall 23200\n",
            "| epoch 058 | valid on 'valid' subset | loss 6.756 | nll_loss 4.501 | ppl 22.64 | num_updates 33060 | best_loss 6.75637\n",
            "| epoch 059 | loss 4.696 | nll_loss 2.106 | ppl 4.31 | wps 17528 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 33630 | lr 0.000344879 | gnorm 0.218 | clip 0.000 | oom 0.000 | wall 16715 | train_wall 23593\n",
            "| epoch 059 | valid on 'valid' subset | loss 6.773 | nll_loss 4.520 | ppl 22.94 | num_updates 33630 | best_loss 6.77268\n",
            "| epoch 060 | loss 4.692 | nll_loss 2.102 | ppl 4.29 | wps 17518 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 34200 | lr 0.000341993 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 17140 | train_wall 23985\n",
            "| epoch 060 | valid on 'valid' subset | loss 6.748 | nll_loss 4.491 | ppl 22.48 | num_updates 34200 | best_loss 6.74759\n",
            "| saved checkpoint checkpoints/checkpoint60.pt (epoch 60 @ 34200 updates) (writing took 5.814113140106201 seconds)\n",
            "| epoch 061 | loss 4.687 | nll_loss 2.096 | ppl 4.28 | wps 17480 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 34770 | lr 0.000339178 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 17571 | train_wall 24379\n",
            "| epoch 061 | valid on 'valid' subset | loss 6.741 | nll_loss 4.488 | ppl 22.44 | num_updates 34770 | best_loss 6.74087\n",
            "| epoch 062 | loss 4.684 | nll_loss 2.092 | ppl 4.26 | wps 17509 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 35340 | lr 0.000336432 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 17996 | train_wall 24771\n",
            "| epoch 062 | valid on 'valid' subset | loss 6.728 | nll_loss 4.461 | ppl 22.02 | num_updates 35340 | best_loss 6.72831\n",
            "| epoch 063 | loss 4.681 | nll_loss 2.088 | ppl 4.25 | wps 17512 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 35910 | lr 0.000333751 | gnorm 0.222 | clip 0.000 | oom 0.000 | wall 18421 | train_wall 25164\n",
            "| epoch 063 | valid on 'valid' subset | loss 6.752 | nll_loss 4.499 | ppl 22.61 | num_updates 35910 | best_loss 6.74759\n",
            "| epoch 064 | loss 4.677 | nll_loss 2.083 | ppl 4.24 | wps 17521 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 36480 | lr 0.000331133 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 18846 | train_wall 25556\n",
            "| epoch 064 | valid on 'valid' subset | loss 6.730 | nll_loss 4.463 | ppl 22.05 | num_updates 36480 | best_loss 6.73037\n",
            "| epoch 065 | loss 4.673 | nll_loss 2.079 | ppl 4.22 | wps 17482 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 37050 | lr 0.000328576 | gnorm 0.220 | clip 0.000 | oom 0.000 | wall 19271 | train_wall 25949\n",
            "| epoch 065 | valid on 'valid' subset | loss 6.722 | nll_loss 4.461 | ppl 22.02 | num_updates 37050 | best_loss 6.72153\n",
            "| epoch 066 | loss 4.670 | nll_loss 2.074 | ppl 4.21 | wps 17448 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 37620 | lr 0.000326077 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 19697 | train_wall 26342\n",
            "| epoch 066 | valid on 'valid' subset | loss 6.740 | nll_loss 4.477 | ppl 22.28 | num_updates 37620 | best_loss 6.74004\n",
            "| epoch 067 | loss 4.667 | nll_loss 2.071 | ppl 4.2 | wps 17410 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 38190 | lr 0.000323635 | gnorm 0.219 | clip 0.000 | oom 0.000 | wall 20125 | train_wall 26736\n",
            "| epoch 067 | valid on 'valid' subset | loss 6.725 | nll_loss 4.464 | ppl 22.06 | num_updates 38190 | best_loss 6.7249\n",
            "| epoch 068 | loss 4.664 | nll_loss 2.067 | ppl 4.19 | wps 17402 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 38760 | lr 0.000321246 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 20552 | train_wall 27130\n",
            "| epoch 068 | valid on 'valid' subset | loss 6.734 | nll_loss 4.472 | ppl 22.19 | num_updates 38760 | best_loss 6.73433\n",
            "| epoch 069 | loss 4.660 | nll_loss 2.062 | ppl 4.17 | wps 17309 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 39330 | lr 0.00031891 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 20982 | train_wall 27525\n",
            "| epoch 069 | valid on 'valid' subset | loss 6.760 | nll_loss 4.507 | ppl 22.74 | num_updates 39330 | best_loss 6.74759\n",
            "| epoch 070 | loss 4.657 | nll_loss 2.058 | ppl 4.16 | wps 17311 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 39900 | lr 0.000316624 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 21412 | train_wall 27921\n",
            "| epoch 070 | valid on 'valid' subset | loss 6.742 | nll_loss 4.486 | ppl 22.41 | num_updates 39900 | best_loss 6.74154\n",
            "| saved checkpoint checkpoints/checkpoint70.pt (epoch 70 @ 39900 updates) (writing took 6.677332162857056 seconds)\n",
            "| epoch 071 | loss 4.654 | nll_loss 2.054 | ppl 4.15 | wps 17334 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 40470 | lr 0.000314386 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 21848 | train_wall 28317\n",
            "| epoch 071 | valid on 'valid' subset | loss 6.767 | nll_loss 4.509 | ppl 22.77 | num_updates 40470 | best_loss 6.74154\n",
            "| epoch 072 | loss 4.650 | nll_loss 2.050 | ppl 4.14 | wps 17381 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 41040 | lr 0.000312195 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 22276 | train_wall 28710\n",
            "| epoch 072 | valid on 'valid' subset | loss 6.709 | nll_loss 4.447 | ppl 21.81 | num_updates 41040 | best_loss 6.70911\n",
            "| epoch 073 | loss 4.647 | nll_loss 2.046 | ppl 4.13 | wps 17374 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 41610 | lr 0.00031005 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 22704 | train_wall 29105\n",
            "| epoch 073 | valid on 'valid' subset | loss 6.740 | nll_loss 4.488 | ppl 22.44 | num_updates 41610 | best_loss 6.74005\n",
            "| epoch 074 | loss 4.645 | nll_loss 2.043 | ppl 4.12 | wps 17364 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 42180 | lr 0.000307948 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 23132 | train_wall 29499\n",
            "| epoch 074 | valid on 'valid' subset | loss 6.736 | nll_loss 4.479 | ppl 22.3 | num_updates 42180 | best_loss 6.73649\n",
            "| epoch 075 | loss 4.642 | nll_loss 2.039 | ppl 4.11 | wps 17456 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 42750 | lr 0.000305888 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 23559 | train_wall 29893\n",
            "| epoch 075 | valid on 'valid' subset | loss 6.716 | nll_loss 4.454 | ppl 21.91 | num_updates 42750 | best_loss 6.71643\n",
            "| epoch 076 | loss 4.639 | nll_loss 2.036 | ppl 4.1 | wps 17467 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 43320 | lr 0.000303869 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 23985 | train_wall 30286\n",
            "| epoch 076 | valid on 'valid' subset | loss 6.738 | nll_loss 4.482 | ppl 22.35 | num_updates 43320 | best_loss 6.73788\n",
            "| epoch 077 | loss 4.636 | nll_loss 2.032 | ppl 4.09 | wps 17459 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 43890 | lr 0.000301889 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 24411 | train_wall 30679\n",
            "| epoch 077 | valid on 'valid' subset | loss 6.736 | nll_loss 4.476 | ppl 22.26 | num_updates 43890 | best_loss 6.73624\n",
            "| epoch 078 | loss 4.634 | nll_loss 2.029 | ppl 4.08 | wps 17457 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 44460 | lr 0.000299948 | gnorm 0.222 | clip 0.000 | oom 0.000 | wall 24837 | train_wall 31073\n",
            "| epoch 078 | valid on 'valid' subset | loss 6.706 | nll_loss 4.456 | ppl 21.94 | num_updates 44460 | best_loss 6.70599\n",
            "| epoch 079 | loss 4.630 | nll_loss 2.025 | ppl 4.07 | wps 17473 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 45030 | lr 0.000298043 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 25263 | train_wall 31466\n",
            "| epoch 079 | valid on 'valid' subset | loss 6.725 | nll_loss 4.465 | ppl 22.08 | num_updates 45030 | best_loss 6.72471\n",
            "| epoch 080 | loss 4.629 | nll_loss 2.023 | ppl 4.06 | wps 17487 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 45600 | lr 0.000296174 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 25688 | train_wall 31859\n",
            "| epoch 080 | valid on 'valid' subset | loss 6.700 | nll_loss 4.448 | ppl 21.83 | num_updates 45600 | best_loss 6.70028\n",
            "| saved checkpoint checkpoints/checkpoint80.pt (epoch 80 @ 45600 updates) (writing took 6.936362266540527 seconds)\n",
            "| epoch 081 | loss 4.626 | nll_loss 2.019 | ppl 4.05 | wps 17468 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 46170 | lr 0.000294341 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 26121 | train_wall 32252\n",
            "| epoch 081 | valid on 'valid' subset | loss 6.724 | nll_loss 4.468 | ppl 22.13 | num_updates 46170 | best_loss 6.70028\n",
            "| epoch 082 | loss 4.623 | nll_loss 2.017 | ppl 4.05 | wps 17501 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 46740 | lr 0.00029254 | gnorm 0.221 | clip 0.000 | oom 0.000 | wall 26546 | train_wall 32645\n",
            "| epoch 082 | valid on 'valid' subset | loss 6.738 | nll_loss 4.481 | ppl 22.34 | num_updates 46740 | best_loss 6.70028\n",
            "| epoch 083 | loss 4.621 | nll_loss 2.014 | ppl 4.04 | wps 17524 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 47310 | lr 0.000290773 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 26971 | train_wall 33037\n",
            "| epoch 083 | valid on 'valid' subset | loss 6.735 | nll_loss 4.484 | ppl 22.39 | num_updates 47310 | best_loss 6.70028\n",
            "| epoch 084 | loss 4.618 | nll_loss 2.010 | ppl 4.03 | wps 17529 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 47880 | lr 0.000289037 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 27395 | train_wall 33430\n",
            "| epoch 084 | valid on 'valid' subset | loss 6.721 | nll_loss 4.467 | ppl 22.12 | num_updates 47880 | best_loss 6.70028\n",
            "| epoch 085 | loss 4.616 | nll_loss 2.007 | ppl 4.02 | wps 17527 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 48450 | lr 0.000287331 | gnorm 0.222 | clip 0.000 | oom 0.000 | wall 27820 | train_wall 33822\n",
            "| epoch 085 | valid on 'valid' subset | loss 6.722 | nll_loss 4.465 | ppl 22.08 | num_updates 48450 | best_loss 6.70028\n",
            "| epoch 086 | loss 4.614 | nll_loss 2.004 | ppl 4.01 | wps 17497 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 49020 | lr 0.000285656 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 28245 | train_wall 34215\n",
            "| epoch 086 | valid on 'valid' subset | loss 6.726 | nll_loss 4.472 | ppl 22.2 | num_updates 49020 | best_loss 6.70028\n",
            "| epoch 087 | loss 4.612 | nll_loss 2.002 | ppl 4.01 | wps 17518 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 49590 | lr 0.00028401 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 28670 | train_wall 34608\n",
            "| epoch 087 | valid on 'valid' subset | loss 6.727 | nll_loss 4.465 | ppl 22.09 | num_updates 49590 | best_loss 6.70028\n",
            "| epoch 088 | loss 4.609 | nll_loss 1.999 | ppl 4 | wps 17545 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 50160 | lr 0.000282391 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 29094 | train_wall 35001\n",
            "| epoch 088 | valid on 'valid' subset | loss 6.707 | nll_loss 4.450 | ppl 21.85 | num_updates 50160 | best_loss 6.70028\n",
            "| epoch 089 | loss 4.607 | nll_loss 1.996 | ppl 3.99 | wps 17520 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 50730 | lr 0.0002808 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 29518 | train_wall 35393\n",
            "| epoch 089 | valid on 'valid' subset | loss 6.734 | nll_loss 4.485 | ppl 22.39 | num_updates 50730 | best_loss 6.70028\n",
            "| epoch 090 | loss 4.604 | nll_loss 1.993 | ppl 3.98 | wps 17492 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 51300 | lr 0.000279236 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 29943 | train_wall 35786\n",
            "| epoch 090 | valid on 'valid' subset | loss 6.711 | nll_loss 4.457 | ppl 21.97 | num_updates 51300 | best_loss 6.70028\n",
            "| saved checkpoint checkpoints/checkpoint90.pt (epoch 90 @ 51300 updates) (writing took 3.6583940982818604 seconds)\n",
            "| epoch 091 | loss 4.602 | nll_loss 1.991 | ppl 3.97 | wps 17504 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 51870 | lr 0.000277697 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 30372 | train_wall 36179\n",
            "| epoch 091 | valid on 'valid' subset | loss 6.742 | nll_loss 4.485 | ppl 22.39 | num_updates 51870 | best_loss 6.70028\n",
            "| epoch 092 | loss 4.600 | nll_loss 1.988 | ppl 3.97 | wps 17533 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 52440 | lr 0.000276184 | gnorm 0.225 | clip 0.000 | oom 0.000 | wall 30796 | train_wall 36571\n",
            "| epoch 092 | valid on 'valid' subset | loss 6.704 | nll_loss 4.442 | ppl 21.74 | num_updates 52440 | best_loss 6.70028\n",
            "| epoch 093 | loss 4.598 | nll_loss 1.985 | ppl 3.96 | wps 17536 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 53010 | lr 0.000274695 | gnorm 0.223 | clip 0.000 | oom 0.000 | wall 31221 | train_wall 36963\n",
            "| epoch 093 | valid on 'valid' subset | loss 6.718 | nll_loss 4.461 | ppl 22.02 | num_updates 53010 | best_loss 6.70028\n",
            "| epoch 094 | loss 4.596 | nll_loss 1.982 | ppl 3.95 | wps 17539 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 53580 | lr 0.00027323 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 31645 | train_wall 37356\n",
            "| epoch 094 | valid on 'valid' subset | loss 6.714 | nll_loss 4.462 | ppl 22.04 | num_updates 53580 | best_loss 6.70028\n",
            "| epoch 095 | loss 4.593 | nll_loss 1.979 | ppl 3.94 | wps 17526 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 54150 | lr 0.000271788 | gnorm 0.224 | clip 0.000 | oom 0.000 | wall 32069 | train_wall 37749\n",
            "| epoch 095 | valid on 'valid' subset | loss 6.713 | nll_loss 4.460 | ppl 22 | num_updates 54150 | best_loss 6.70028\n",
            "| epoch 096 | loss 4.592 | nll_loss 1.978 | ppl 3.94 | wps 17527 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 54720 | lr 0.000270369 | gnorm 0.226 | clip 0.000 | oom 0.000 | wall 32494 | train_wall 38141\n",
            "| epoch 096 | valid on 'valid' subset | loss 6.716 | nll_loss 4.457 | ppl 21.97 | num_updates 54720 | best_loss 6.70028\n",
            "| epoch 097 | loss 4.590 | nll_loss 1.976 | ppl 3.93 | wps 17543 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 55290 | lr 0.000268972 | gnorm 0.225 | clip 0.000 | oom 0.000 | wall 32918 | train_wall 38533\n",
            "| epoch 097 | valid on 'valid' subset | loss 6.746 | nll_loss 4.485 | ppl 22.4 | num_updates 55290 | best_loss 6.70028\n",
            "| epoch 098 | loss 4.589 | nll_loss 1.973 | ppl 3.93 | wps 17538 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 55860 | lr 0.000267596 | gnorm 0.225 | clip 0.000 | oom 0.000 | wall 33342 | train_wall 38925\n",
            "| epoch 098 | valid on 'valid' subset | loss 6.681 | nll_loss 4.427 | ppl 21.51 | num_updates 55860 | best_loss 6.68093\n",
            "| epoch 099 | loss 4.586 | nll_loss 1.970 | ppl 3.92 | wps 17602 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 56430 | lr 0.000266241 | gnorm 0.226 | clip 0.000 | oom 0.000 | wall 33765 | train_wall 39317\n",
            "| epoch 099 | valid on 'valid' subset | loss 6.737 | nll_loss 4.485 | ppl 22.4 | num_updates 56430 | best_loss 6.70028\n",
            "| epoch 100 | loss 4.584 | nll_loss 1.967 | ppl 3.91 | wps 17529 | ups 1 | wpb 12991.582 | bsz 989.086 | num_updates 57000 | lr 0.000264906 | gnorm 0.225 | clip 0.000 | oom 0.000 | wall 34189 | train_wall 39709\n",
            "| epoch 100 | valid on 'valid' subset | loss 6.699 | nll_loss 4.451 | ppl 21.87 | num_updates 57000 | best_loss 6.69884\n",
            "| saved checkpoint checkpoints/checkpoint100.pt (epoch 100 @ 57000 updates) (writing took 6.25425910949707 seconds)\n",
            "| done training in 34188.0 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2SfpeHfqt00",
        "colab_type": "text"
      },
      "source": [
        "Train en-ne using this command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8gqY9LWqpDx",
        "colab_type": "code",
        "outputId": "80607847-60c5-4c3c-fb62-2160968a7a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
        "    data-bin/wiki_ne_en_bpe5000/ \\\n",
        "    --source-lang en --target-lang ne \\\n",
        "    --arch transformer --share-all-embeddings \\\n",
        "    --encoder-layers 5 --decoder-layers 5 \\\n",
        "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
        "    --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
        "    --encoder-attention-heads 2 --decoder-attention-heads 2 \\\n",
        "    --encoder-normalize-before --decoder-normalize-before \\\n",
        "    --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --label-smoothing 0.2 --criterion label_smoothed_cross_entropy \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \\\n",
        "    --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \\\n",
        "    --lr 1e-3 --min-lr 1e-9 \\\n",
        "    --max-tokens 4000 \\\n",
        "    --update-freq 4 \\\n",
        "    --max-epoch 100 --save-interval 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation_dropout=0.2, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.2, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wiki_ne_en_bpe5000/', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=2, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=5, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.4, empty_cache_freq=0, encoder_attention_heads=2, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=5, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.2, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=10, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='ne', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)\n",
            "| [en] dictionary: 5000 types\n",
            "| [ne] dictionary: 5000 types\n",
            "| loaded 2559 examples from: data-bin/wiki_ne_en_bpe5000/valid.ne-en.en\n",
            "| loaded 2559 examples from: data-bin/wiki_ne_en_bpe5000/valid.ne-en.ne\n",
            "| data-bin/wiki_ne_en_bpe5000/ valid en-ne 2559 examples\n",
            "TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(5000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(5000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "| model transformer, criterion LabelSmoothedCrossEntropyCriterion\n",
            "| num. model params: 39344128 (num. trained: 39344128)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 4000 and max sentences per GPU = None\n",
            "| loaded checkpoint checkpoints/checkpoint_last.pt (epoch 40 @ 22960 updates)\n",
            "| loading train data for epoch 40\n",
            "| loaded 563779 examples from: data-bin/wiki_ne_en_bpe5000/train.ne-en.en\n",
            "| loaded 563779 examples from: data-bin/wiki_ne_en_bpe5000/train.ne-en.ne\n",
            "| data-bin/wiki_ne_en_bpe5000/ train en-ne 563779 examples\n",
            "| epoch 041 | loss 5.087 | nll_loss 2.586 | ppl 6.01 | wps 16335 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 23534 | lr 0.00041227 | gnorm 0.229 | clip 0.000 | oom 0.000 | wall 473 | train_wall 25467\n",
            "| epoch 041 | valid on 'valid' subset | loss 7.539 | nll_loss 5.483 | ppl 44.71 | num_updates 23534 | best_loss 7.53855\n",
            "| epoch 042 | loss 5.080 | nll_loss 2.578 | ppl 5.97 | wps 16231 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 24108 | lr 0.000407333 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 940 | train_wall 25883\n",
            "| epoch 042 | valid on 'valid' subset | loss 7.536 | nll_loss 5.484 | ppl 44.77 | num_updates 24108 | best_loss 7.53625\n",
            "| epoch 043 | loss 5.072 | nll_loss 2.567 | ppl 5.93 | wps 16289 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 24682 | lr 0.000402569 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 1405 | train_wall 26299\n",
            "| epoch 043 | valid on 'valid' subset | loss 7.538 | nll_loss 5.486 | ppl 44.81 | num_updates 24682 | best_loss 7.53846\n",
            "| epoch 044 | loss 5.065 | nll_loss 2.558 | ppl 5.89 | wps 16247 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 25256 | lr 0.000397968 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 1872 | train_wall 26715\n",
            "| epoch 044 | valid on 'valid' subset | loss 7.543 | nll_loss 5.493 | ppl 45.03 | num_updates 25256 | best_loss 7.54323\n",
            "| epoch 045 | loss 5.059 | nll_loss 2.552 | ppl 5.86 | wps 16242 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 25830 | lr 0.000393521 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 2339 | train_wall 27132\n",
            "| epoch 045 | valid on 'valid' subset | loss 7.545 | nll_loss 5.490 | ppl 44.94 | num_updates 25830 | best_loss 7.54464\n",
            "| epoch 046 | loss 5.052 | nll_loss 2.542 | ppl 5.82 | wps 16263 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 26404 | lr 0.00038922 | gnorm 0.229 | clip 0.000 | oom 0.000 | wall 2806 | train_wall 27549\n",
            "| epoch 046 | valid on 'valid' subset | loss 7.570 | nll_loss 5.512 | ppl 45.63 | num_updates 26404 | best_loss 7.56988\n",
            "| epoch 047 | loss 5.045 | nll_loss 2.533 | ppl 5.79 | wps 16199 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 26978 | lr 0.000385057 | gnorm 0.229 | clip 0.000 | oom 0.000 | wall 3274 | train_wall 27966\n",
            "| epoch 047 | valid on 'valid' subset | loss 7.546 | nll_loss 5.493 | ppl 45.04 | num_updates 26978 | best_loss 7.54646\n",
            "| epoch 048 | loss 5.040 | nll_loss 2.527 | ppl 5.76 | wps 16205 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 27552 | lr 0.000381025 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 3742 | train_wall 28383\n",
            "| epoch 048 | valid on 'valid' subset | loss 7.545 | nll_loss 5.488 | ppl 44.87 | num_updates 27552 | best_loss 7.54504\n",
            "| epoch 049 | loss 5.034 | nll_loss 2.518 | ppl 5.73 | wps 16209 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 28126 | lr 0.000377117 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 4210 | train_wall 28800\n",
            "| epoch 049 | valid on 'valid' subset | loss 7.538 | nll_loss 5.481 | ppl 44.68 | num_updates 28126 | best_loss 7.53827\n",
            "| epoch 050 | loss 5.027 | nll_loss 2.510 | ppl 5.7 | wps 16190 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 28700 | lr 0.000373327 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 4678 | train_wall 29217\n",
            "| epoch 050 | valid on 'valid' subset | loss 7.529 | nll_loss 5.473 | ppl 44.41 | num_updates 28700 | best_loss 7.52851\n",
            "| saved checkpoint checkpoints/checkpoint50.pt (epoch 50 @ 28700 updates) (writing took 14.925277948379517 seconds)\n",
            "| epoch 051 | loss 5.023 | nll_loss 2.505 | ppl 5.68 | wps 16157 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 29274 | lr 0.000369649 | gnorm 0.229 | clip 0.000 | oom 0.000 | wall 5163 | train_wall 29634\n",
            "| epoch 051 | valid on 'valid' subset | loss 7.528 | nll_loss 5.468 | ppl 44.27 | num_updates 29274 | best_loss 7.5279\n",
            "| epoch 052 | loss 5.017 | nll_loss 2.498 | ppl 5.65 | wps 16222 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 29848 | lr 0.000366077 | gnorm 0.230 | clip 0.000 | oom 0.000 | wall 5630 | train_wall 30051\n",
            "| epoch 052 | valid on 'valid' subset | loss 7.504 | nll_loss 5.444 | ppl 43.52 | num_updates 29848 | best_loss 7.5043\n",
            "| epoch 053 | loss 5.012 | nll_loss 2.491 | ppl 5.62 | wps 16180 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 30422 | lr 0.000362607 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 6099 | train_wall 30468\n",
            "| epoch 053 | valid on 'valid' subset | loss 7.537 | nll_loss 5.475 | ppl 44.47 | num_updates 30422 | best_loss 7.52851\n",
            "| epoch 054 | loss 5.007 | nll_loss 2.485 | ppl 5.6 | wps 16200 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 30996 | lr 0.000359234 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 6567 | train_wall 30886\n",
            "| epoch 054 | valid on 'valid' subset | loss 7.509 | nll_loss 5.439 | ppl 43.39 | num_updates 30996 | best_loss 7.50915\n",
            "| epoch 055 | loss 5.001 | nll_loss 2.477 | ppl 5.57 | wps 16141 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 31570 | lr 0.000355953 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 7037 | train_wall 31304\n",
            "| epoch 055 | valid on 'valid' subset | loss 7.483 | nll_loss 5.421 | ppl 42.84 | num_updates 31570 | best_loss 7.48313\n",
            "| epoch 056 | loss 4.997 | nll_loss 2.472 | ppl 5.55 | wps 16153 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 32144 | lr 0.000352761 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 7507 | train_wall 31722\n",
            "| epoch 056 | valid on 'valid' subset | loss 7.491 | nll_loss 5.427 | ppl 43.03 | num_updates 32144 | best_loss 7.491\n",
            "| epoch 057 | loss 4.992 | nll_loss 2.466 | ppl 5.53 | wps 16178 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 32718 | lr 0.000349652 | gnorm 0.233 | clip 0.000 | oom 0.000 | wall 7975 | train_wall 32140\n",
            "| epoch 057 | valid on 'valid' subset | loss 7.506 | nll_loss 5.443 | ppl 43.5 | num_updates 32718 | best_loss 7.50612\n",
            "| epoch 058 | loss 4.988 | nll_loss 2.460 | ppl 5.5 | wps 16154 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 33292 | lr 0.000346625 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 8445 | train_wall 32557\n",
            "| epoch 058 | valid on 'valid' subset | loss 7.514 | nll_loss 5.451 | ppl 43.74 | num_updates 33292 | best_loss 7.51406\n",
            "| epoch 059 | loss 4.984 | nll_loss 2.455 | ppl 5.48 | wps 16105 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 33866 | lr 0.000343675 | gnorm 0.233 | clip 0.000 | oom 0.000 | wall 8916 | train_wall 32975\n",
            "| epoch 059 | valid on 'valid' subset | loss 7.518 | nll_loss 5.454 | ppl 43.83 | num_updates 33866 | best_loss 7.51839\n",
            "| epoch 060 | loss 4.980 | nll_loss 2.450 | ppl 5.47 | wps 16165 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 34440 | lr 0.000340799 | gnorm 0.233 | clip 0.000 | oom 0.000 | wall 9385 | train_wall 33393\n",
            "| epoch 060 | valid on 'valid' subset | loss 7.534 | nll_loss 5.468 | ppl 44.27 | num_updates 34440 | best_loss 7.52851\n",
            "| saved checkpoint checkpoints/checkpoint60.pt (epoch 60 @ 34440 updates) (writing took 4.693482875823975 seconds)\n",
            "| epoch 061 | loss 4.975 | nll_loss 2.444 | ppl 5.44 | wps 16091 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 35014 | lr 0.000337994 | gnorm 0.232 | clip 0.000 | oom 0.000 | wall 9861 | train_wall 33812\n",
            "| epoch 061 | valid on 'valid' subset | loss 7.508 | nll_loss 5.442 | ppl 43.47 | num_updates 35014 | best_loss 7.50832\n",
            "| epoch 062 | loss 4.971 | nll_loss 2.438 | ppl 5.42 | wps 16142 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 35588 | lr 0.000335257 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 10331 | train_wall 34231\n",
            "| epoch 062 | valid on 'valid' subset | loss 7.490 | nll_loss 5.418 | ppl 42.74 | num_updates 35588 | best_loss 7.4903\n",
            "| epoch 063 | loss 4.967 | nll_loss 2.434 | ppl 5.41 | wps 16139 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 36162 | lr 0.000332586 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 10801 | train_wall 34649\n",
            "| epoch 063 | valid on 'valid' subset | loss 7.501 | nll_loss 5.441 | ppl 43.44 | num_updates 36162 | best_loss 7.50053\n",
            "| epoch 064 | loss 4.963 | nll_loss 2.429 | ppl 5.38 | wps 16130 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 36736 | lr 0.000329977 | gnorm 0.232 | clip 0.000 | oom 0.000 | wall 11271 | train_wall 35068\n",
            "| epoch 064 | valid on 'valid' subset | loss 7.488 | nll_loss 5.421 | ppl 42.84 | num_updates 36736 | best_loss 7.4879\n",
            "| epoch 065 | loss 4.960 | nll_loss 2.425 | ppl 5.37 | wps 16143 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 37310 | lr 0.000327429 | gnorm 0.231 | clip 0.000 | oom 0.000 | wall 11741 | train_wall 35486\n",
            "| epoch 065 | valid on 'valid' subset | loss 7.499 | nll_loss 5.427 | ppl 43.03 | num_updates 37310 | best_loss 7.49911\n",
            "| epoch 066 | loss 4.955 | nll_loss 2.419 | ppl 5.35 | wps 16160 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 37884 | lr 0.000324939 | gnorm 0.240 | clip 0.000 | oom 0.000 | wall 12210 | train_wall 35904\n",
            "| epoch 066 | valid on 'valid' subset | loss 7.461 | nll_loss 5.395 | ppl 42.07 | num_updates 37884 | best_loss 7.46115\n",
            "| epoch 067 | loss 4.951 | nll_loss 2.414 | ppl 5.33 | wps 16161 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 38458 | lr 0.000322505 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 12679 | train_wall 36323\n",
            "| epoch 067 | valid on 'valid' subset | loss 7.498 | nll_loss 5.428 | ppl 43.06 | num_updates 38458 | best_loss 7.49824\n",
            "| epoch 068 | loss 4.948 | nll_loss 2.409 | ppl 5.31 | wps 16166 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 39032 | lr 0.000320125 | gnorm 0.232 | clip 0.000 | oom 0.000 | wall 13149 | train_wall 36741\n",
            "| epoch 068 | valid on 'valid' subset | loss 7.476 | nll_loss 5.402 | ppl 42.29 | num_updates 39032 | best_loss 7.4762\n",
            "| epoch 069 | loss 4.944 | nll_loss 2.405 | ppl 5.29 | wps 16154 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 39606 | lr 0.000317797 | gnorm 0.232 | clip 0.000 | oom 0.000 | wall 13618 | train_wall 37159\n",
            "| epoch 069 | valid on 'valid' subset | loss 7.502 | nll_loss 5.433 | ppl 43.21 | num_updates 39606 | best_loss 7.5019\n",
            "| epoch 070 | loss 4.942 | nll_loss 2.401 | ppl 5.28 | wps 16100 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 40180 | lr 0.000315519 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 14089 | train_wall 37578\n",
            "| epoch 070 | valid on 'valid' subset | loss 7.504 | nll_loss 5.439 | ppl 43.39 | num_updates 40180 | best_loss 7.50381\n",
            "| saved checkpoint checkpoints/checkpoint70.pt (epoch 70 @ 40180 updates) (writing took 8.014426469802856 seconds)\n",
            "| epoch 071 | loss 4.938 | nll_loss 2.397 | ppl 5.27 | wps 16007 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 40754 | lr 0.000313289 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 14571 | train_wall 37998\n",
            "| epoch 071 | valid on 'valid' subset | loss 7.460 | nll_loss 5.394 | ppl 42.05 | num_updates 40754 | best_loss 7.45978\n",
            "| epoch 072 | loss 4.935 | nll_loss 2.393 | ppl 5.25 | wps 16118 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 41328 | lr 0.000311106 | gnorm 0.233 | clip 0.000 | oom 0.000 | wall 15042 | train_wall 38417\n",
            "| epoch 072 | valid on 'valid' subset | loss 7.460 | nll_loss 5.395 | ppl 42.06 | num_updates 41328 | best_loss 7.4596\n",
            "| epoch 073 | loss 4.931 | nll_loss 2.388 | ppl 5.23 | wps 16145 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 41902 | lr 0.000308967 | gnorm 0.233 | clip 0.000 | oom 0.000 | wall 15511 | train_wall 38836\n",
            "| epoch 073 | valid on 'valid' subset | loss 7.456 | nll_loss 5.385 | ppl 41.79 | num_updates 41902 | best_loss 7.45634\n",
            "| epoch 074 | loss 4.928 | nll_loss 2.385 | ppl 5.22 | wps 16102 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 42476 | lr 0.000306873 | gnorm 0.236 | clip 0.000 | oom 0.000 | wall 15982 | train_wall 39254\n",
            "| epoch 074 | valid on 'valid' subset | loss 7.491 | nll_loss 5.428 | ppl 43.05 | num_updates 42476 | best_loss 7.49072\n",
            "| epoch 075 | loss 4.925 | nll_loss 2.380 | ppl 5.21 | wps 16082 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 43050 | lr 0.00030482 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 16454 | train_wall 39673\n",
            "| epoch 075 | valid on 'valid' subset | loss 7.462 | nll_loss 5.386 | ppl 41.83 | num_updates 43050 | best_loss 7.46187\n",
            "| epoch 076 | loss 4.922 | nll_loss 2.377 | ppl 5.19 | wps 16131 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 43624 | lr 0.000302808 | gnorm 0.235 | clip 0.000 | oom 0.000 | wall 16924 | train_wall 40092\n",
            "| epoch 076 | valid on 'valid' subset | loss 7.467 | nll_loss 5.396 | ppl 42.11 | num_updates 43624 | best_loss 7.46675\n",
            "| epoch 077 | loss 4.918 | nll_loss 2.372 | ppl 5.18 | wps 16242 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 44198 | lr 0.000300835 | gnorm 0.235 | clip 0.000 | oom 0.000 | wall 17391 | train_wall 40509\n",
            "| epoch 077 | valid on 'valid' subset | loss 7.472 | nll_loss 5.399 | ppl 42.19 | num_updates 44198 | best_loss 7.47163\n",
            "| epoch 078 | loss 4.916 | nll_loss 2.369 | ppl 5.17 | wps 16455 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 44772 | lr 0.000298901 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 17852 | train_wall 40921\n",
            "| epoch 078 | valid on 'valid' subset | loss 7.447 | nll_loss 5.380 | ppl 41.65 | num_updates 44772 | best_loss 7.4472\n",
            "| epoch 079 | loss 4.913 | nll_loss 2.366 | ppl 5.15 | wps 16479 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 45346 | lr 0.000297003 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 18312 | train_wall 41332\n",
            "| epoch 079 | valid on 'valid' subset | loss 7.447 | nll_loss 5.376 | ppl 41.54 | num_updates 45346 | best_loss 7.44728\n",
            "| epoch 080 | loss 4.910 | nll_loss 2.362 | ppl 5.14 | wps 16502 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 45920 | lr 0.000295141 | gnorm 0.236 | clip 0.000 | oom 0.000 | wall 18771 | train_wall 41744\n",
            "| epoch 080 | valid on 'valid' subset | loss 7.493 | nll_loss 5.424 | ppl 42.93 | num_updates 45920 | best_loss 7.49299\n",
            "| saved checkpoint checkpoints/checkpoint80.pt (epoch 80 @ 45920 updates) (writing took 7.6827898025512695 seconds)\n",
            "| epoch 081 | loss 4.907 | nll_loss 2.358 | ppl 5.13 | wps 16481 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 46494 | lr 0.000293313 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 19239 | train_wall 42156\n",
            "| epoch 081 | valid on 'valid' subset | loss 7.452 | nll_loss 5.380 | ppl 41.64 | num_updates 46494 | best_loss 7.4521\n",
            "| epoch 082 | loss 4.905 | nll_loss 2.354 | ppl 5.11 | wps 16516 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 47068 | lr 0.000291519 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 19698 | train_wall 42567\n",
            "| epoch 082 | valid on 'valid' subset | loss 7.461 | nll_loss 5.387 | ppl 41.86 | num_updates 47068 | best_loss 7.46085\n",
            "| epoch 083 | loss 4.902 | nll_loss 2.350 | ppl 5.1 | wps 16501 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 47642 | lr 0.000289758 | gnorm 0.236 | clip 0.000 | oom 0.000 | wall 20158 | train_wall 42979\n",
            "| epoch 083 | valid on 'valid' subset | loss 7.455 | nll_loss 5.384 | ppl 41.76 | num_updates 47642 | best_loss 7.45474\n",
            "| epoch 084 | loss 4.899 | nll_loss 2.347 | ppl 5.09 | wps 16439 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 48216 | lr 0.000288028 | gnorm 0.239 | clip 0.000 | oom 0.000 | wall 20619 | train_wall 43391\n",
            "| epoch 084 | valid on 'valid' subset | loss 7.473 | nll_loss 5.400 | ppl 42.21 | num_updates 48216 | best_loss 7.47349\n",
            "| epoch 085 | loss 4.896 | nll_loss 2.343 | ppl 5.08 | wps 16396 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 48790 | lr 0.000286329 | gnorm 0.235 | clip 0.000 | oom 0.000 | wall 21082 | train_wall 43805\n",
            "| epoch 085 | valid on 'valid' subset | loss 7.475 | nll_loss 5.406 | ppl 42.41 | num_updates 48790 | best_loss 7.47502\n",
            "| epoch 086 | loss 4.894 | nll_loss 2.341 | ppl 5.07 | wps 16407 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 49364 | lr 0.000284659 | gnorm 0.236 | clip 0.000 | oom 0.000 | wall 21544 | train_wall 44218\n",
            "| epoch 086 | valid on 'valid' subset | loss 7.466 | nll_loss 5.392 | ppl 42 | num_updates 49364 | best_loss 7.46648\n",
            "| epoch 087 | loss 4.891 | nll_loss 2.337 | ppl 5.05 | wps 16371 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 49938 | lr 0.000283018 | gnorm 0.236 | clip 0.000 | oom 0.000 | wall 22007 | train_wall 44631\n",
            "| epoch 087 | valid on 'valid' subset | loss 7.431 | nll_loss 5.363 | ppl 41.14 | num_updates 49938 | best_loss 7.4309\n",
            "| epoch 088 | loss 4.890 | nll_loss 2.335 | ppl 5.05 | wps 16407 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 50512 | lr 0.000281406 | gnorm 0.237 | clip 0.000 | oom 0.000 | wall 22469 | train_wall 45044\n",
            "| epoch 088 | valid on 'valid' subset | loss 7.447 | nll_loss 5.374 | ppl 41.48 | num_updates 50512 | best_loss 7.44723\n",
            "| epoch 089 | loss 4.886 | nll_loss 2.330 | ppl 5.03 | wps 16415 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 51086 | lr 0.00027982 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 22931 | train_wall 45457\n",
            "| epoch 089 | valid on 'valid' subset | loss 7.481 | nll_loss 5.400 | ppl 42.23 | num_updates 51086 | best_loss 7.48119\n",
            "| epoch 090 | loss 4.884 | nll_loss 2.328 | ppl 5.02 | wps 16451 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 51660 | lr 0.000278261 | gnorm 0.237 | clip 0.000 | oom 0.000 | wall 23392 | train_wall 45870\n",
            "| epoch 090 | valid on 'valid' subset | loss 7.460 | nll_loss 5.385 | ppl 41.8 | num_updates 51660 | best_loss 7.4596\n",
            "| saved checkpoint checkpoints/checkpoint90.pt (epoch 90 @ 51660 updates) (writing took 8.073248863220215 seconds)\n",
            "| epoch 091 | loss 4.882 | nll_loss 2.325 | ppl 5.01 | wps 16393 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 52234 | lr 0.000276728 | gnorm 0.237 | clip 0.000 | oom 0.000 | wall 23863 | train_wall 46283\n",
            "| epoch 091 | valid on 'valid' subset | loss 7.444 | nll_loss 5.369 | ppl 41.34 | num_updates 52234 | best_loss 7.44392\n",
            "| epoch 092 | loss 4.880 | nll_loss 2.323 | ppl 5.01 | wps 16362 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 52808 | lr 0.00027522 | gnorm 0.237 | clip 0.000 | oom 0.000 | wall 24326 | train_wall 46697\n",
            "| epoch 092 | valid on 'valid' subset | loss 7.440 | nll_loss 5.369 | ppl 41.32 | num_updates 52808 | best_loss 7.43979\n",
            "| epoch 093 | loss 4.877 | nll_loss 2.319 | ppl 4.99 | wps 16331 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 53382 | lr 0.000273736 | gnorm 0.237 | clip 0.000 | oom 0.000 | wall 24790 | train_wall 47111\n",
            "| epoch 093 | valid on 'valid' subset | loss 7.472 | nll_loss 5.397 | ppl 42.14 | num_updates 53382 | best_loss 7.4596\n",
            "| epoch 094 | loss 4.876 | nll_loss 2.317 | ppl 4.98 | wps 16413 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 53956 | lr 0.000272276 | gnorm 0.239 | clip 0.000 | oom 0.000 | wall 25252 | train_wall 47524\n",
            "| epoch 094 | valid on 'valid' subset | loss 7.448 | nll_loss 5.380 | ppl 41.63 | num_updates 53956 | best_loss 7.44841\n",
            "| epoch 095 | loss 4.873 | nll_loss 2.313 | ppl 4.97 | wps 16425 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 54530 | lr 0.00027084 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 25714 | train_wall 47936\n",
            "| epoch 095 | valid on 'valid' subset | loss 7.455 | nll_loss 5.385 | ppl 41.78 | num_updates 54530 | best_loss 7.45506\n",
            "| epoch 096 | loss 4.870 | nll_loss 2.311 | ppl 4.96 | wps 16416 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 55104 | lr 0.000269425 | gnorm 0.239 | clip 0.000 | oom 0.000 | wall 26176 | train_wall 48349\n",
            "| epoch 096 | valid on 'valid' subset | loss 7.458 | nll_loss 5.385 | ppl 41.78 | num_updates 55104 | best_loss 7.45765\n",
            "| epoch 097 | loss 4.868 | nll_loss 2.308 | ppl 4.95 | wps 16404 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 55678 | lr 0.000268033 | gnorm 0.240 | clip 0.000 | oom 0.000 | wall 26638 | train_wall 48762\n",
            "| epoch 097 | valid on 'valid' subset | loss 7.436 | nll_loss 5.358 | ppl 41.01 | num_updates 55678 | best_loss 7.43639\n",
            "| epoch 098 | loss 4.866 | nll_loss 2.305 | ppl 4.94 | wps 16414 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 56252 | lr 0.000266662 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 27100 | train_wall 49175\n",
            "| epoch 098 | valid on 'valid' subset | loss 7.447 | nll_loss 5.375 | ppl 41.49 | num_updates 56252 | best_loss 7.44681\n",
            "| epoch 099 | loss 4.865 | nll_loss 2.303 | ppl 4.94 | wps 16439 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 56826 | lr 0.000265312 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 27562 | train_wall 49587\n",
            "| epoch 099 | valid on 'valid' subset | loss 7.480 | nll_loss 5.409 | ppl 42.49 | num_updates 56826 | best_loss 7.4596\n",
            "| epoch 100 | loss 4.862 | nll_loss 2.299 | ppl 4.92 | wps 16394 | ups 1 | wpb 13148.371 | bsz 982.193 | num_updates 57400 | lr 0.000263982 | gnorm 0.239 | clip 0.000 | oom 0.000 | wall 28024 | train_wall 50000\n",
            "| epoch 100 | valid on 'valid' subset | loss 7.470 | nll_loss 5.399 | ppl 42.19 | num_updates 57400 | best_loss 7.4596\n",
            "| saved checkpoint checkpoints/checkpoint100.pt (epoch 100 @ 57400 updates) (writing took 4.717652797698975 seconds)\n",
            "| done training in 28021.1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}