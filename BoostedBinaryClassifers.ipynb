{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BoostedBinaryClassifers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCec51FAfHWJulOMX3iQkK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvaBansal00/ConfidentMT/blob/master/BoostedBinaryClassifers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEk99K4f0Hw_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "d31aab3d-c9e9-4b6b-adcf-76a528d4bf8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls\n",
        "%cd drive/My Drive/ConfidentMachineTranslation/flores\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n",
            "/content/drive/My Drive/ConfidentMachineTranslation/flores\n",
            "analysis\t\t       data\t\t       LM_Thresholding.ipynb\n",
            "Analysis\t\t       data-bin\t\t       NCD_Analysis.ipynb\n",
            "backward_models\t\t       Ensembles\t       noisychannel\n",
            "BoostedBinaryClassifers.ipynb  Ensembling\t       NoisyChannel.ipynb\n",
            "checkpoints\t\t       FLORES.ipynb\t       Resources\n",
            "ClassificationDataset\t       LanguageAnalysis.ipynb  scripts\n",
            "configs\t\t\t       language_models\n",
            "/content/drive/My Drive/ConfidentMachineTranslation/flores\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlcgFl162kdH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "42370734-45db-4752-b8f7-55126b2475a6"
      },
      "source": [
        "# prints how much GPU RAM is available\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=11a6390ea47ac81e8d42a0e29cbf3e6f2703217c9a8c091e6bd9b8eccf2fca4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 159.4 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xwd7lNk3EZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "499647a0-8f64-4183-f45b-f2bd24fd9a23"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from itertools import zip_longest\n",
        "\n",
        "def printDatasetClassProp(Y): \n",
        "    classes = {}\n",
        "    total = len(Y)\n",
        "    for i in Y:\n",
        "        if i in classes:\n",
        "            classes[i] += 1\n",
        "        else:\n",
        "            classes[i] = 1\n",
        "    \n",
        "    for cls in classes:\n",
        "        print(\"Proportion in class \" + str(cls) + \" = \" + str(classes[cls]/total))\n",
        "\n",
        "def datasetReader(featureFile, labelFile):\n",
        "    files = [featureFile, labelFile]\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for lines in zip_longest(*files, fillvalue=''):\n",
        "        currX, currY = lines[0], float(lines[1].strip(\"\\n\"))\n",
        "        Xarr = []\n",
        "        features = currX.split()\n",
        "        for feature in features:\n",
        "            Xarr.append(float(feature.strip(\",\").strip(\"\\n\")))\n",
        "        X.append(Xarr)\n",
        "        Y.append(currY)\n",
        "    \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def trainRandomForestClassifier(X, Y, maxDepth):\n",
        "    print(\"Training Random Forrest Classifier\")\n",
        "    clf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def calculateAccuracy(predictedClasses, groundTruth):\n",
        "    correct_accepted = 0\n",
        "    total_accepted = 0\n",
        "\n",
        "    correct_rejected = 0\n",
        "    total_rejected = 0\n",
        "\n",
        "    for i in range(len(predictedClasses)):\n",
        "        if groundTruth[i] == 1:\n",
        "            total_accepted += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_accepted += 1\n",
        "        else:\n",
        "            total_rejected += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_rejected += 1\n",
        "\n",
        "\n",
        "    print(\"Correctly accepted = \" + str(correct_accepted/total_accepted))\n",
        "    print(\"Incorrectly accepted = \" + str(1 - correct_rejected/total_rejected))\n",
        "    print(\"Correctly rejected = \" + str(correct_rejected/total_rejected))\n",
        "    print(\"Incorrectly rejected = \" + str(1 - correct_accepted/total_accepted))\n",
        "\n",
        "    print(\"Total Accuracy = \" + str((correct_accepted + correct_rejected)/(total_accepted + total_rejected)))\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+trainset+\"/features.txt\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+testset+\"/features.txt\")\n",
        "\n",
        "trainLabels = open(\"ClassificationDataset/\"+trainset+\"/result.txt\")\n",
        "testLabels = open(\"ClassificationDataset/\"+testset+\"/result.txt\")\n",
        "\n",
        "trainX, trainY = datasetReader(trainFeatures, trainLabels)\n",
        "testX, testY = datasetReader(testFeatures, testLabels)\n",
        "\n",
        "printDatasetClassProp(trainY)\n",
        "\n",
        "\n",
        "\n",
        "rndForest = trainRandomForestClassifier(trainX, trainY, 8)\n",
        "predictions = np.array(rndForest.predict(testX))\n",
        "calculateAccuracy(predictions, testY)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportion in class 0.0 = 0.8014849550605705\n",
            "Proportion in class 1.0 = 0.19851504493942945\n",
            "Training Random Forrest Classifier\n",
            "Correctly accepted = 0.24200278164116829\n",
            "Incorrectly accepted = 0.03308128544423439\n",
            "Correctly rejected = 0.9669187145557656\n",
            "Incorrectly rejected = 0.7579972183588317\n",
            "Total Accuracy = 0.783068783068783\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}