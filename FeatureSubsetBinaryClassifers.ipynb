{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureSubsetBinaryClassifers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvaBansal00/ConfidentMT/blob/master/FeatureSubsetBinaryClassifers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEk99K4f0Hw_",
        "colab_type": "code",
        "outputId": "3cb81169-b612-402b-e6f2-6f4dbb804b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls\n",
        "%cd drive/My Drive/ConfidentMachineTranslation/flores\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n",
            "/content/drive/.shortcut-targets-by-id/1viBwZM7BIiD8O4LeU-9UQXyV82oTWqh6/ConfidentMachineTranslation/flores\n",
            "'=0.5.0'\t\t\t FeatureSubsetBinaryClassifers.ipynb\n",
            " analysis\t\t\t FLORES.ipynb\n",
            " Analysis\t\t\t LanguageAnalysis.ipynb\n",
            " backward_models\t\t language_models\n",
            " BoostedBinaryClassifers.ipynb\t LM_Thresholding.ipynb\n",
            " checkpoints\t\t\t NCD_Analysis.ipynb\n",
            " ClassificationDataset\t\t NNClassification.ipynb\n",
            " configs\t\t\t noisychannel\n",
            " data\t\t\t\t NoisyChannel.ipynb\n",
            " data-bin\t\t\t Resources\n",
            " Ensembles\t\t\t scripts\n",
            " Ensembling\n",
            "/content/drive/.shortcut-targets-by-id/1viBwZM7BIiD8O4LeU-9UQXyV82oTWqh6/ConfidentMachineTranslation/flores\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0LVqC0npSH1",
        "colab_type": "code",
        "outputId": "bd53f708-aadf-48e0-9429-fd35acad2e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "pip install fairseq sacrebleu sentencepiece tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 2.8MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021175 sha256=9047c2304b6c0fa9eb5e6d3ab5aaabfbf92e042b3c2b25db06874c9de3c1a541\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq, sentencepiece\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.9 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlcgFl162kdH",
        "colab_type": "code",
        "outputId": "979505d1-7df1-4040-a3d2-439341875637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# prints how much GPU RAM is available\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=10857f23a888d24a4b262c7b5e05d9d9c35290bcee328055d0e8b1094eaa6755\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 193.3 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xwd7lNk3EZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import numpy as np\n",
        "from itertools import zip_longest\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "class CustomEnsembleClassifier:\n",
        "    def __init__(self, clfs):\n",
        "        self.classifiers = clfs\n",
        "    \n",
        "    def predict(self, X):\n",
        "        probabilities = None\n",
        "        for clf in self.classifiers:\n",
        "            if probabilities is None:\n",
        "                probabilities = clf.predict_proba(X)\n",
        "            else:\n",
        "                probabilities += clf.predict_proba(X)\n",
        "        return np.argmax(np.array(probabilities), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "def printDatasetClassProp(Y): \n",
        "    classes = {}\n",
        "    total = len(Y)\n",
        "    for i in Y:\n",
        "        if i in classes:\n",
        "            classes[i] += 1\n",
        "        else:\n",
        "            classes[i] = 1\n",
        "    \n",
        "    for cls in classes:\n",
        "        print(\"Proportion in class \" + str(cls) + \" = \" + str(classes[cls]/total))\n",
        "\n",
        "def datasetReader(featureFile, labelFile):\n",
        "    files = [featureFile, labelFile]\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for lines in zip_longest(*files, fillvalue=''):\n",
        "        currX, currY = lines[0], float(lines[1].strip(\"\\n\"))\n",
        "        Xarr = []\n",
        "        features = currX.split()\n",
        "        for feature in features:\n",
        "            Xarr.append(float(feature.strip(\",\").strip(\"\\n\")))\n",
        "        X.append(Xarr)\n",
        "        Y.append(currY)    \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def computeSimilarity(o1, o2):\n",
        "    total = len(o1)\n",
        "    same = 0\n",
        "    for i in range(len(o1)):\n",
        "        if o1[i] == o2[i]:\n",
        "            same += 1\n",
        "    print(same/total)\n",
        "\n",
        "\n",
        "def trainMLPClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training MLP Classifier\")\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(64, 256, 512, 256, 64), random_state=42,\n",
        "                        max_iter=200, learning_rate='adaptive', learning_rate_init=0.0005, activation='relu')\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainKNeighborsClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training KNeighbors Classifier\")\n",
        "    clf = KNeighborsClassifier(100)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainGaussianProcessClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Gaussian Process Classifier\")\n",
        "    length_scale = [1 for i in range(len(X[0]))]\n",
        "    clf = GaussianProcessClassifier(1.0 * RBF(length_scale), warm_start=True, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainCustomEnsemble(X, Y, maxDepth=8, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training custom ensemble\")\n",
        "    rf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    grad = GradientBoostingClassifier(random_state=42)\n",
        "    ada = AdaBoostClassifier(n_estimators=estimators, random_state=42)\n",
        "    # dl = MLPClassifier(hidden_layer_sizes=(100), random_state=1, max_iter=200)\n",
        "    # kn = KNeighborsClassifier(100)\n",
        "\n",
        "    classifiers = [rf, grad, ada]\n",
        "\n",
        "    for clf in classifiers:\n",
        "        clf.fit(X, Y)\n",
        "\n",
        "    return CustomEnsembleClassifier(classifiers)\n",
        "    \n",
        "\n",
        "def trainEnsembleClassifier(X, Y, maxDepth=8, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training an ensemble of Random Forest and Gradient Boosting Classifiers\")\n",
        "\n",
        "    estimators = [\n",
        "     ('rf', RandomForestClassifier(max_depth=maxDepth, random_state=42)),\n",
        "     ('grad', GradientBoostingClassifier(random_state=42))]\n",
        "    clf = StackingClassifier(estimators=estimators, final_estimator=AdaBoostClassifier(n_estimators=50, random_state=42))\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def trainRandomForestClassifier(X, Y, maxDepth=8, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Random Forest classifier\")\n",
        "    clf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainAdaBoostClassifier(X, Y, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training AdaBoosted Decision Tree classifier\")\n",
        "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=estimators, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainGradientBoostingClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Graident Boosted classifier\")\n",
        "    clf = GradientBoostingClassifier(random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainSVM(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training SVM classifier\")\n",
        "    clf = SVC(gamma='auto')\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def calculateAccuracy(predictedClasses, groundTruth):\n",
        "    correct_accepted = 0\n",
        "    total_accepted = 0\n",
        "\n",
        "    correct_rejected = 0\n",
        "    total_rejected = 0\n",
        "\n",
        "    for i in range(len(predictedClasses)):\n",
        "        if groundTruth[i] == 1:\n",
        "            total_accepted += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_accepted += 1\n",
        "        else:\n",
        "            total_rejected += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_rejected += 1\n",
        "\n",
        "\n",
        "    print(\"Correctly accepted = \" + str(correct_accepted/total_accepted))\n",
        "    print(\"Incorrectly rejected = \" + str(1 - correct_accepted/total_accepted))\n",
        "    print(\"Correctly rejected = \" + str(correct_rejected/total_rejected))\n",
        "    print(\"Incorrectly accepted = \" + str(1 - correct_rejected/total_rejected))\n",
        "\n",
        "    print(\"Total Accuracy = \" + str((correct_accepted + correct_rejected)/(total_accepted + total_rejected)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_4LpbZ6UBsX",
        "colab_type": "code",
        "outputId": "3212535b-cff1-4faa-cd9c-927622743543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "##make precision graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "import tqdm\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    if len(acceptedTranslations) != 0:\n",
        "        temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.data\", \"w\")\n",
        "        temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.data\", \"w\")\n",
        "\n",
        "    \n",
        "        for translation in acceptedTranslations:\n",
        "            temporary_reference_inclusion.write(translation.reference)\n",
        "            temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_inclusion.data --ref analysis/temporary_reference_inclusion.data --sacrebleu > analysis/inclusion_result.data\n",
        "\n",
        "        temporary_inclusion_result = open(\"analysis/inclusion_result.data\")\n",
        "        inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "        temporary_inclusion_result.close()\n",
        "\n",
        "    else:\n",
        "        inclusion_result_string = \"0\"\n",
        "\n",
        "    if len(rejectedTranslations) != 0:\n",
        "\n",
        "        temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.data\", \"w\")\n",
        "        temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.data\", \"w\")\n",
        "        \n",
        "        for translation in rejectedTranslations:\n",
        "            temporary_reference_exclusion.write(translation.reference)\n",
        "            temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "        \n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_exclusion.data --ref analysis/temporary_reference_exclusion.data --sacrebleu > analysis/exclusion_result.data\n",
        "\n",
        "        temporary_exclusion_result = open(\"analysis/exclusion_result.data\")\n",
        "        exclusion_result_string = \"0\" if len(rejectedTranslations) == 0 else [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "\n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "        temporary_exclusion_result.close()\n",
        "    \n",
        "    else:\n",
        "        exclusion_result_string = \"0\"\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "# featuresUsed = [0, 5, 6, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20]\n",
        "# featuresUsed = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] ##All\n",
        "avgLogProb = [False, False, False, False]\n",
        "featuresUsed = [0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "# featuresUsed = [10, 11, 12]\n",
        "featureSubsets = [[10], #just baseline Forward Model score [P(T|S)]\n",
        "                  [10, 11, 12], #Similar to noisy channel decoding: P(y|x), p(x|y), p(y) \n",
        "                  [10, 11, 12, 19, 20], #Like above + sentence length features\n",
        "                  [0, 10, 11, 12, 19, 20], #Like above + average Logprob + sentence length features\n",
        "                  [0, 5, 6, 10, 11, 12, 19, 20], #Like above + average Logprob + sentence length features + Rare words\n",
        "                  [0, 10, 11, 12, 13, 14, 19, 20], #Like above + average Logprob + sentence length features + end of sentence identifiers\n",
        "                  [10, 11, 12, 16, 17, 18], #Like above + ngram features\n",
        "                  [0, 10, 11, 12, 16, 17, 18], #Like above + average Logprob + ngram features\n",
        "                  [0, 5, 6, 10, 11, 12, 13, 14, 19, 20] #Like above + average Logprob + sentence length features + Rare words + end of sentence identifiers\n",
        "                  ]\n",
        "featureSubsetDetails = [\"Just baseline Forward Model score [P(T|S)]\",\n",
        "                        \"Similar to noisy channel decoding: P(y|x), p(x|y), p(y)\",\n",
        "                        \"NCD features + sentence length features\",\n",
        "                        \"NCD features + average Logprob + sentence length features\",\n",
        "                        \"NCD features + average Logprob + sentence length features + Rare words\",\n",
        "                        \"NCD features + average Logprob + sentence length features + end of sentence identifiers\",\n",
        "                        \"NCD features + ngram features\",\n",
        "                        \"NCD features + average Logprob + ngram features\",\n",
        "                        \"NCD features + average Logprob + sentence length features + Rare words + end of sentence identifiers\"]\n",
        "\n",
        "trainThresholds = [np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "testThresholds = [np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "modelLabel = {0: \"Random Forest Classifier\", 1: \"Custom Ensemble\", 2: \"Gradient Boosting Classifier\", 3: \"MLP Classifier\"}\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.data\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.data\")\n",
        "\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.data\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.data\")\n",
        "\n",
        "trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "fullTrainX, fullTrainY = datasetReader(trainFeatures, trainLabels)\n",
        "fullTestX, fullTestY = datasetReader(testFeatures, testLabels)\n",
        "featuresTrain = fullTrainX\n",
        "featuresTest = fullTestX\n",
        "print(featuresTrain.shape)\n",
        "# print(len(trainX[0]))\n",
        "# print(len(testX[0]))\n",
        "\n",
        "print(np.array(fullTrainX).shape)\n",
        "print(np.array(fullTestX).shape)\n",
        "print(\"TRAIN SET CLASS PROPORTIONS:\")\n",
        "printDatasetClassProp(fullTrainY)\n",
        "print(\"TEST SET CLASS PROPORTIONS\")\n",
        "printDatasetClassProp(fullTestY)\n",
        "print()\n",
        "classifiers = [trainMLPClassifier]\n",
        "'''\n",
        "for ind, subset in enumerate(featureSubsets):\n",
        "  print(featureSubsetDetails[ind])\n",
        "  trainX = [[row[i] for i in subset] for row in fullTrainX]\n",
        "  testX = [[row[i] for i in subset] for row in fullTestX]\n",
        "\n",
        "  print(np.array(trainX).shape)\n",
        "  print(np.array(testX).shape)\n",
        "  classifiers = [trainRandomForestClassifier, trainCustomEnsemble, trainGradientBoostingClassifier] #trainMLPClassifier]\n",
        "  outputs = []\n",
        "  models = []\n",
        "  plt.xlabel('Fraction Above Threshold') \n",
        "  plt.ylabel('Corpus BLEU score') \n",
        "  plt.title('Comparing Methods using Corpus BLEU score')\n",
        "  for j, classifier in enumerate(classifiers):\n",
        "      print(\"#################################################\")\n",
        "      curr = classifier(trainX, fullTrainY)\n",
        "      print(\"TRAIN ACCURACY\")\n",
        "      predictions = np.array(curr.predict(trainX))\n",
        "      calculateAccuracy(predictions, fullTrainY)\n",
        "      print(\"TEST ACCURACY\")\n",
        "      predictions = np.array(curr.predict(testX))\n",
        "      calculateAccuracy(predictions, fullTestY)\n",
        "      outputs.append(predictions)\n",
        "      models.append(curr)\n",
        "      print(\"#################################################\")\n",
        "      currFeaturesTrain = [[row[i] for i in featureSubsets[ind]] for row in featuresTrain]\n",
        "      currFeaturesTest = [[row[i] for i in featureSubsets[ind]] for row in featuresTest]\n",
        "      trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "      testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "\n",
        "      trainTranslations = readTranslations(trainSentences, currFeaturesTrain)\n",
        "      testTranslations = readTranslations(testSentences, currFeaturesTest)\n",
        "\n",
        "      acceptedScores = []\n",
        "      acceptedFraction = []\n",
        "      print(\"TRAIN SET\")\n",
        "      for index in tqdm.tqdm(range(len(testThresholds[j]))):\n",
        "          trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[j][index], testThresholds[j][index], avgLogProb[j])\n",
        "          # print(len(trainY))\n",
        "          clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "          predictions = clf.predict(trainFeatures)\n",
        "          \n",
        "          acceptedTranslations = np.array(trainTranslations)[np.array(predictions) > 0]\n",
        "          rejectedTranslations = np.array(trainTranslations)[np.array(predictions) < 1]\n",
        "            \n",
        "          rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "          \n",
        "          acceptedScores.append(acceptedScore)\n",
        "          acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "      \n",
        "      r = random.random()\n",
        "      b = random.random()\n",
        "      g = random.random()\n",
        "      c = (r, g, b)\n",
        "      plt.plot(acceptedFraction, acceptedScores, label = modelLabel[j], color=c)\n",
        "      acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "      acceptedFraction.sort()\n",
        "\n",
        "      print(\"[\"+modelLabel[j]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "      print(\"TEST SET\")\n",
        "      for index in tqdm.tqdm(range(len(testThresholds[j]))):\n",
        "          trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[j][index], testThresholds[j][index], avgLogProb[j])\n",
        "          # print(len(trainY))\n",
        "          clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "          predictions = clf.predict(testFeatures)\n",
        "          \n",
        "          acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "          rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "            \n",
        "          rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "          \n",
        "          acceptedScores.append(acceptedScore)\n",
        "          acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "      \n",
        "      r = random.random()\n",
        "      b = random.random()\n",
        "      g = random.random()\n",
        "      c = (r, g, b)\n",
        "      plt.plot(acceptedFraction, acceptedScores, label = modelLabel[j], color=c)\n",
        "      acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "      acceptedFraction.sort()\n",
        "\n",
        "      print(\"[\"+modelLabel[j]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "for output_1 in outputs:\n",
        "    for output_2 in outputs:\n",
        "        computeSimilarity(output_1, output_2)'''\n",
        "\n",
        "'''\n",
        "trainFeatures.close()\n",
        "trainLabels.close()\n",
        "testFeatures.close()\n",
        "testLabels.close()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2559, 23)\n",
            "(2559, 23)\n",
            "(2835, 23)\n",
            "TRAIN SET CLASS PROPORTIONS:\n",
            "Proportion in class 0.0 = 0.8014849550605705\n",
            "Proportion in class 1.0 = 0.19851504493942945\n",
            "TEST SET CLASS PROPORTIONS\n",
            "Proportion in class 1.0 = 0.2536155202821869\n",
            "Proportion in class 0.0 = 0.7463844797178131\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrainFeatures.close()\\ntrainLabels.close()\\ntestFeatures.close()\\ntestLabels.close()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccsFgQUiD656",
        "colab_type": "code",
        "outputId": "3c0ca593-d3bc-4a0b-90a6-12e6e35bdf56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "\n",
        "dataset = \"valid\"\n",
        "bleuThreshold = 15\n",
        "featureFile = open(\"ClassificationDataset/\"+str(bleuThreshold)+\"BLEU/\"+dataset+\"/features.data\")\n",
        "labelFile = open(\"ClassificationDataset/\"+str(bleuThreshold)+\"BLEU/\"+dataset+\"/result.data\")\n",
        "featuresUsed = [10]\n",
        "\n",
        "inputs, labels = datasetReader(featureFile, labelFile)\n",
        "for classifier in classifiers:\n",
        "  features = np.array([[row[i] for i in featuresUsed] for row in inputs])\n",
        "  kf = KFold(n_splits=len(features))\n",
        "\n",
        "  numCorrect = 0\n",
        "  currIter = 0\n",
        "  numCorrectTrain = 0\n",
        "  for train_index, test_index in kf.split(features):\n",
        "      #print(\"Currently done with \" + str(currIter)+\"/\"+str(len(features)))\n",
        "      trainX, trainY = features[train_index], labels[train_index]\n",
        "      testX, testY = features[test_index], labels[test_index]\n",
        "      if currIter == 0:\n",
        "        curr = classifier(trainX, trainY, verbose=True)\n",
        "      else:\n",
        "        curr = classifier(trainX, trainY, verbose=False)\n",
        "      \n",
        "      prediction = np.array(curr.predict(trainX))\n",
        "      if prediction[0] == trainY[0]:\n",
        "        numCorrectTrain += 1\n",
        "      \n",
        "      prediction = np.array(curr.predict(testX))\n",
        "\n",
        "      if prediction[0] == testY[0]:\n",
        "          numCorrect += 1\n",
        "      \n",
        "      currIter += 1\n",
        "      #print(\"Current Accuracy = \" + str(float(numCorrect)/float(currIter)))\n",
        "\n",
        "  print(\"Total Train Accuracy = \" + str(numCorrectTrain/len(features)))\n",
        "  print(\"Total Test Accuracy = \" + str(numCorrect/len(features)))\n",
        "\n",
        "featureFile.close()\n",
        "labelFile.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MLP Classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1uY1tRPfEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##make precision graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.data\", \"w\")\n",
        "    temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.data\", \"w\")\n",
        "\n",
        "    temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.data\", \"w\")\n",
        "    temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.data\", \"w\")\n",
        "    \n",
        "    for translation in acceptedTranslations:\n",
        "        temporary_reference_inclusion.write(translation.reference)\n",
        "        temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "    for translation in rejectedTranslations:\n",
        "        temporary_reference_exclusion.write(translation.reference)\n",
        "        temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "    \n",
        "    temporary_reference_inclusion.close()\n",
        "    temporary_output_inclusion.close()\n",
        "    temporary_reference_exclusion.close()\n",
        "    temporary_output_exclusion.close()\n",
        "\n",
        "    !fairseq-score --sys analysis/temporary_output_inclusion.data --ref analysis/temporary_reference_inclusion.data --sacrebleu > analysis/inclusion_result.data\n",
        "    !fairseq-score --sys analysis/temporary_output_exclusion.data --ref analysis/temporary_reference_exclusion.data --sacrebleu > analysis/exclusion_result.data\n",
        "\n",
        "    temporary_inclusion_result = open(\"analysis/inclusion_result.data\")\n",
        "    temporary_exclusion_result = open(\"analysis/exclusion_result.data\")\n",
        "    inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "    exclusion_result_string = [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "# featuresUsed = [0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "# featuresUsed = [0]\n",
        "# featuresUsed = [0, 4]\n",
        "\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.data\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.data\")\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.data\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.data\")\n",
        "\n",
        "trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "\n",
        "\n",
        "featuresTrain, _ = datasetReader(trainFeatures, trainLabels)\n",
        "featuresTest, _ = datasetReader(testFeatures, testLabels)\n",
        "\n",
        "featuresTrain = [[row[i] for i in featuresUsed] for row in featuresTrain]\n",
        "featuresTest = [[row[i] for i in featuresUsed] for row in featuresTest]\n",
        "\n",
        "trainTranslations = readTranslations(trainSentences, featuresTrain)\n",
        "testTranslations = readTranslations(testSentences, featuresTest)\n",
        "\n",
        "# Thresholds_train = np.linspace(-1.5, 0, 25).tolist()\n",
        "Thresholds_train = np.linspace(4, 28, 25).tolist()\n",
        "\n",
        "Thresholds_test = np.linspace(4, 28, 25).tolist()\n",
        "\n",
        "acceptedScores = []\n",
        "acceptedFraction = []\n",
        "\n",
        "useSentenceBLEUScore = True\n",
        "\n",
        "\n",
        "for index in range(len(Thresholds_test)):\n",
        "    trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, Thresholds_train[index], Thresholds_test[index])\n",
        "\n",
        "    clf = trainSVM(trainFeatures, trainY)\n",
        "    # print(\"Using Average Logprob Decision Stump of \" + str(Thresholds_train[index]))\n",
        "    # print(\"BLEU score = \" + str(Thresholds_test[index]))\n",
        "    predictions = clf.predict(testFeatures)\n",
        "    # calculateAccuracy(predictions, testY)\n",
        "    # print(\"##########################################\")\n",
        "    acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "    rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "    if useSentenceBLEUScore:\n",
        "        rejectedScore, acceptedScore = compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations)\n",
        "    else:\n",
        "        rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "    acceptedScores.append(acceptedScore)\n",
        "    acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "\n",
        "plt.xlabel('Fraction Above Threshold') \n",
        "plt.ylabel('BLEU score (average)') \n",
        "plt.title('Random Forest Thresholding') \n",
        "\n",
        "r = random.random()\n",
        "b = random.random()\n",
        "g = random.random()\n",
        "c = (r, g, b)\n",
        "plt.scatter(acceptedFraction, acceptedScores, label = \"Random Forest Analysis\", color=c)\n",
        "\n",
        "acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "acceptedFraction.sort()\n",
        "\n",
        "print('AUC for incuded fraction: {}'.format(auc(acceptedFraction, acceptedScores)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utOMembzJXAc",
        "colab_type": "text"
      },
      "source": [
        "Experimenting with multiple parameters and plotting curves on the same graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJFjlR8sJtkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    if len(acceptedTranslations) != 0:\n",
        "        temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.data\", \"w\")\n",
        "        temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.data\", \"w\")\n",
        "\n",
        "    \n",
        "        for translation in acceptedTranslations:\n",
        "            temporary_reference_inclusion.write(translation.reference)\n",
        "            temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_inclusion.data --ref analysis/temporary_reference_inclusion.data --sacrebleu > analysis/inclusion_result.data\n",
        "\n",
        "        temporary_inclusion_result = open(\"analysis/inclusion_result.data\")\n",
        "        inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    else:\n",
        "        inclusion_result_string = \"0\"\n",
        "\n",
        "    if len(rejectedTranslations) != 0:\n",
        "\n",
        "        temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.data\", \"w\")\n",
        "        temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.data\", \"w\")\n",
        "        \n",
        "        for translation in rejectedTranslations:\n",
        "            temporary_reference_exclusion.write(translation.reference)\n",
        "            temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "        \n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_exclusion.data --ref analysis/temporary_reference_exclusion.data --sacrebleu > analysis/exclusion_result.data\n",
        "\n",
        "        temporary_exclusion_result = open(\"analysis/exclusion_result.data\")\n",
        "        exclusion_result_string = \"0\" if len(rejectedTranslations) == 0 else [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "    \n",
        "    else:\n",
        "        exclusion_result_string = \"0\"\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 10\n",
        "bleuThresholdTest = 10\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.data\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.data\")\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.data\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.data\")\n",
        "\n",
        "\n",
        "featuresTrain, _ = datasetReader(trainFeatures, trainLabels)\n",
        "featuresTest, _ = datasetReader(testFeatures, testLabels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ItLAR-5J-ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featuresUsed = [[0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [0], [4]]\n",
        "trainThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(-1.5, -0.25, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "testThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(4, 28, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "\n",
        "evaluationMetrics = [compute_exclued_included_sentenceBleuScore, compute_excluded_included_score]\n",
        "evalLabel = {0: \"Average Sentence BLEU score\", 1: \"Corpus BLEU score\"}\n",
        "models = [trainRandomForestClassifier, trainRandomForestClassifier, trainRandomForestClassifier]\n",
        "modelLabel = {0: \"Random Forest Classifier (all features)\", 1: \"Average Logprob Thresholding\", 2: \"Sentence BLEU Thresholding\"}\n",
        "avgLogProb = [False, True, False]\n",
        "acceptedScores = []\n",
        "acceptedFraction = []\n",
        "\n",
        "\n",
        "for evalMetric in range(len(evaluationMetrics)):\n",
        "    plt.xlabel('Fraction Above Threshold') \n",
        "    plt.ylabel(evalLabel[evalMetric]) \n",
        "    plt.title('Comparing Methods using ' + evalLabel[evalMetric])\n",
        "\n",
        "    for model in range(len(models)):\n",
        "        currFeaturesTrain = [[row[i] for i in featuresUsed[model]] for row in featuresTrain]\n",
        "        currFeaturesTest = [[row[i] for i in featuresUsed[model]] for row in featuresTest]\n",
        "\n",
        "\n",
        "        trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "        testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "\n",
        "        trainTranslations = readTranslations(trainSentences, currFeaturesTrain)\n",
        "        testTranslations = readTranslations(testSentences, currFeaturesTest)\n",
        "\n",
        "        acceptedScores = []\n",
        "        acceptedFraction = []\n",
        "\n",
        "        for index in range(len(testThresholds[model])):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[model][index], testThresholds[model][index], avgLogProb[model])\n",
        "            clf = models[model](trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(testFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "              \n",
        "            rejectedScore, acceptedScore = evaluationMetrics[evalMetric](acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[model], color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "\n",
        "        print(\"[\"+modelLabel[model]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}