{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureSubsetBinaryClassifers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b6752a23b08d4e41ad2eee4b7d7fd6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_298116b581c14e41b8d9eed0798bdd1b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_35e5a5a37d3c4378bda986a906981cea",
              "IPY_MODEL_f571e13bd70e4e478ce2ced2082d82f6"
            ]
          }
        },
        "298116b581c14e41b8d9eed0798bdd1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35e5a5a37d3c4378bda986a906981cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1f16184ce08b4615ac4cfb2a24c5b274",
            "_dom_classes": [],
            "description": " 72%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 18,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f0b2344d45148ebb8fa8c61cdc85770"
          }
        },
        "f571e13bd70e4e478ce2ced2082d82f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cab0d3e7847846be82206cb97fd87f59",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 18/25 [01:31&lt;00:29,  4.23s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa2cab37e43e4e90acc0b717098c6b39"
          }
        },
        "1f16184ce08b4615ac4cfb2a24c5b274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f0b2344d45148ebb8fa8c61cdc85770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cab0d3e7847846be82206cb97fd87f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa2cab37e43e4e90acc0b717098c6b39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvaBansal00/ConfidentMT/blob/master/FeatureSubsetBinaryClassifers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEk99K4f0Hw_",
        "colab_type": "code",
        "outputId": "f5eca437-abb4-4c2b-ea3d-eaae73a98650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls\n",
        "%cd drive/My Drive/ConfidentMachineTranslation/flores\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n",
            "/content/drive/My Drive/ConfidentMachineTranslation/flores\n",
            "analysis\t\t       FeatureSubsetBinaryClassifers.ipynb\n",
            "Analysis\t\t       FLORES.ipynb\n",
            "backward_models\t\t       LanguageAnalysis.ipynb\n",
            "BoostedBinaryClassifers.ipynb  language_models\n",
            "checkpoints\t\t       LM_Thresholding.ipynb\n",
            "ClassificationDataset\t       NCD_Analysis.ipynb\n",
            "configs\t\t\t       NNClassification.ipynb\n",
            "data\t\t\t       noisychannel\n",
            "data-bin\t\t       NoisyChannel.ipynb\n",
            "Ensembles\t\t       Resources\n",
            "Ensembling\t\t       scripts\n",
            "/content/drive/My Drive/ConfidentMachineTranslation/flores\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0LVqC0npSH1",
        "colab_type": "code",
        "outputId": "637514f2-9ec3-49bf-8cce-14d250f2551b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "pip install fairseq sacrebleu sentencepiece tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 6.9MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021159 sha256=27c75f5828308a5cb7a72e92de69d5d09fd55612d7b718f338d614b5228767fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq, sentencepiece\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.10 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlcgFl162kdH",
        "colab_type": "code",
        "outputId": "900e30ce-593c-4f96-c16b-450c7b679c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# prints how much GPU RAM is available\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=5bb713c6ad58933edbe989846e7e0c5f02b55c84a5c23e54b44a95101d9c28e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 160.1 MB\n",
            "GPU RAM Free: 7611MB | Used: 0MB | Util   0% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xwd7lNk3EZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import numpy as np\n",
        "from itertools import zip_longest\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "class CustomEnsembleClassifier:\n",
        "    def __init__(self, clfs):\n",
        "        self.classifiers = clfs\n",
        "    \n",
        "    def predict(self, X):\n",
        "        probabilities = None\n",
        "        for clf in self.classifiers:\n",
        "            if probabilities is None:\n",
        "                probabilities = clf.predict_proba(X)\n",
        "            else:\n",
        "                probabilities += clf.predict_proba(X)\n",
        "        return np.argmax(np.array(probabilities), axis=1)\n",
        "\n",
        "\n",
        "def printDatasetClassProp(Y): \n",
        "    classes = {}\n",
        "    total = len(Y)\n",
        "    for i in Y:\n",
        "        if i in classes:\n",
        "            classes[i] += 1\n",
        "        else:\n",
        "            classes[i] = 1\n",
        "    \n",
        "    for cls in classes:\n",
        "        print(\"Proportion in class \" + str(cls) + \" = \" + str(classes[cls]/total))\n",
        "\n",
        "def datasetReader(featureFile, labelFile):\n",
        "    files = [featureFile, labelFile]\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for lines in zip_longest(*files, fillvalue=''):\n",
        "        currX, currY = lines[0], float(lines[1].strip(\"\\n\"))\n",
        "        Xarr = []\n",
        "        features = currX.split()\n",
        "        for feature in features:\n",
        "            Xarr.append(float(feature.strip(\",\").strip(\"\\n\")))\n",
        "        X.append(Xarr)\n",
        "        Y.append(currY)    \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def computeSimilarity(o1, o2):\n",
        "    total = len(o1)\n",
        "    same = 0\n",
        "    for i in range(len(o1)):\n",
        "        if o1[i] == o2[i]:\n",
        "            same += 1\n",
        "    print(same/total)\n",
        "\n",
        "\n",
        "def trainLogisticRegressionClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Traning Logistic Regression Classifier\")\n",
        "    clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def trainMLPClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training MLP Classifier\")\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(64, 256, 512, 256, 64), random_state=42,\n",
        "                        max_iter=200, learning_rate='adaptive', learning_rate_init=0.0005, activation='relu')\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainKNeighborsClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training KNeighbors Classifier\")\n",
        "    clf = KNeighborsClassifier(100)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainGaussianProcessClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Gaussian Process Classifier\")\n",
        "    length_scale = [1 for i in range(len(X[0]))]\n",
        "    clf = GaussianProcessClassifier(1.0 * RBF(length_scale), warm_start=True, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainCustomEnsemble(X, Y, maxDepth=8, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training custom ensemble\")\n",
        "    rf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    grad = GradientBoostingClassifier(random_state=42)\n",
        "    ada = AdaBoostClassifier(n_estimators=estimators, random_state=42)\n",
        "    # dl = MLPClassifier(hidden_layer_sizes=(100), random_state=1, max_iter=200)\n",
        "    # kn = KNeighborsClassifier(100)\n",
        "\n",
        "    classifiers = [rf, grad, ada]\n",
        "\n",
        "    for clf in classifiers:\n",
        "        clf.fit(X, Y)\n",
        "\n",
        "    return CustomEnsembleClassifier(classifiers)\n",
        "    \n",
        "\n",
        "def trainEnsembleClassifier(X, Y, maxDepth=8, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training an ensemble of Random Forest and Gradient Boosting Classifiers\")\n",
        "\n",
        "    estimators = [\n",
        "     ('rf', RandomForestClassifier(max_depth=maxDepth, random_state=42)),\n",
        "     ('grad', GradientBoostingClassifier(random_state=42))]\n",
        "    clf = StackingClassifier(estimators=estimators, final_estimator=AdaBoostClassifier(n_estimators=50, random_state=42))\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def trainRandomForestClassifier(X, Y, maxDepth=8, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Random Forest classifier\")\n",
        "    clf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainAdaBoostClassifier(X, Y, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training AdaBoosted Decision Tree classifier\")\n",
        "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=estimators, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainGradientBoostingClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Graident Boosted classifier\")\n",
        "    clf = GradientBoostingClassifier(random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainSVM(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training SVM classifier\")\n",
        "    clf = SVC(gamma='auto')\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def calculateAccuracy(predictedClasses, groundTruth):\n",
        "    correct_accepted = 0\n",
        "    total_accepted = 0\n",
        "\n",
        "    correct_rejected = 0\n",
        "    total_rejected = 0\n",
        "\n",
        "    for i in range(len(predictedClasses)):\n",
        "        if groundTruth[i] == 1:\n",
        "            total_accepted += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_accepted += 1\n",
        "        else:\n",
        "            total_rejected += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_rejected += 1\n",
        "\n",
        "\n",
        "    print(\"Correctly accepted = \" + str(correct_accepted/total_accepted))\n",
        "    print(\"Incorrectly rejected = \" + str(1 - correct_accepted/total_accepted))\n",
        "    print(\"Correctly rejected = \" + str(correct_rejected/total_rejected))\n",
        "    print(\"Incorrectly accepted = \" + str(1 - correct_rejected/total_rejected))\n",
        "\n",
        "    print(\"Total Accuracy = \" + str((correct_accepted + correct_rejected)/(total_accepted + total_rejected)))\n",
        "\n",
        "def calculatedAcceptedFraction(predictedClasses):\n",
        "    return len([i for i in predictedClasses if i > 0])/len(predictedClasses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_4LpbZ6UBsX",
        "colab_type": "code",
        "outputId": "495049b9-7157-431f-81e0-49c568b2c122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588,
          "referenced_widgets": [
            "b6752a23b08d4e41ad2eee4b7d7fd6e2",
            "298116b581c14e41b8d9eed0798bdd1b",
            "35e5a5a37d3c4378bda986a906981cea",
            "f571e13bd70e4e478ce2ced2082d82f6",
            "1f16184ce08b4615ac4cfb2a24c5b274",
            "1f0b2344d45148ebb8fa8c61cdc85770",
            "cab0d3e7847846be82206cb97fd87f59",
            "aa2cab37e43e4e90acc0b717098c6b39"
          ]
        }
      },
      "source": [
        "##make precision graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    if len(acceptedTranslations) != 0:\n",
        "        temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.data\", \"w\")\n",
        "        temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.data\", \"w\")\n",
        "\n",
        "    \n",
        "        for translation in acceptedTranslations:\n",
        "            temporary_reference_inclusion.write(translation.reference)\n",
        "            temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_inclusion.data --ref analysis/temporary_reference_inclusion.data --sacrebleu > analysis/inclusion_result.data\n",
        "\n",
        "        temporary_inclusion_result = open(\"analysis/inclusion_result.data\")\n",
        "        inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "        temporary_inclusion_result.close()\n",
        "\n",
        "    else:\n",
        "        inclusion_result_string = \"0\"\n",
        "\n",
        "    if len(rejectedTranslations) != 0:\n",
        "\n",
        "        temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.data\", \"w\")\n",
        "        temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.data\", \"w\")\n",
        "        \n",
        "        for translation in rejectedTranslations:\n",
        "            temporary_reference_exclusion.write(translation.reference)\n",
        "            temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "        \n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_exclusion.data --ref analysis/temporary_reference_exclusion.data --sacrebleu > analysis/exclusion_result.data\n",
        "\n",
        "        temporary_exclusion_result = open(\"analysis/exclusion_result.data\")\n",
        "        exclusion_result_string = \"0\" if len(rejectedTranslations) == 0 else [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "\n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "        temporary_exclusion_result.close()\n",
        "    \n",
        "    else:\n",
        "        exclusion_result_string = \"0\"\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "# featuresUsed = [0, 5, 6, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20]\n",
        "# featuresUsed = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] ##All\n",
        "avgLogProb = [False, False, False, False]\n",
        "featuresUsed = [0, 1, 2, 3, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "# featuresUsed = [10, 11, 12]\n",
        "featureSubsets = [[0], #just baseline Forward Model score [P(T|S)]\n",
        "                  [0, 11, 12], #Similar to noisy channel decoding: P(y|x), p(x|y), p(y) \n",
        "                  [0, 11, 12, 19, 20], #Like above + sentence length features\n",
        "                  [0, 11, 12, 16, 17, 18], #Like above + ngram features\n",
        "                  [0, 5, 6, 11, 12, 19, 20], #Like above + sentence length features + Rare words\n",
        "                  [0, 11, 12, 13, 14, 19, 20], #Like above + sentence length features + end of sentence identifiers\n",
        "                  [0, 5, 6, 11, 12, 16, 17, 18], #Like above + ngram features + Rare words\n",
        "                  [0, 11, 12, 13, 14, 16, 17, 18], #Like above + ngram features + end of sentence identifiers\n",
        "                  [0, 5, 6, 11, 12, 13, 14, 19, 20], #Like above + sentence length features + Rare words + end of sentence identifiers\n",
        "                  [0, 5, 6, 11, 12, 13, 14, 16, 17, 18] #Like above + ngram features + Rare words + end of sentence identifiers\n",
        "                  ]\n",
        "featureSubsetDetails = [\"Just Forward Model score P(y|x)\",\n",
        "                        \"NCD features: P(y|x), p(x|y), p(y)\",\n",
        "                        \"NCD features + sentence length features\",\n",
        "                        \"NCD features + ngram features\",\n",
        "                        \"NCD features + sentence length features + Rare words\",\n",
        "                        \"NCD features + sentence length features + end of sentence identifiers\",\n",
        "                        \"NCD features + ngram features + Rare words\",\n",
        "                        \"NCD features + ngram features + end of sentence identifiers\",\n",
        "                        \"NCD features + sentence length features + Rare words + end of sentence identifiers\",\n",
        "                        \"NCD features + ngram features + Rare words + end of sentence identifiers\"]\n",
        "\n",
        "trainThresholds = [np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "testThresholds = [np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "modelLabel = {0: \"Logistic Regression Classifier\", 1: \"Custom Ensemble\", 2: \"Gradient Boosting Classifier\", 3: \"MLP Classifier\"}\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.data\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.data\")\n",
        "\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.data\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.data\")\n",
        "\n",
        "trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "\n",
        "fullTrainX, fullTrainY = datasetReader(trainFeatures, trainLabels)\n",
        "fullTestX, fullTestY = datasetReader(testFeatures, testLabels)\n",
        "featuresTrain = fullTrainX\n",
        "featuresTest = fullTestX\n",
        "print(featuresTrain.shape)\n",
        "# print(len(trainX[0]))\n",
        "# print(len(testX[0]))\n",
        "\n",
        "print(np.array(fullTrainX).shape)\n",
        "print(np.array(fullTestX).shape)\n",
        "print(\"TRAIN SET CLASS PROPORTIONS:\")\n",
        "printDatasetClassProp(fullTrainY)\n",
        "print(\"TEST SET CLASS PROPORTIONS\")\n",
        "printDatasetClassProp(fullTestY)\n",
        "print()\n",
        "classifiers = [trainLogisticRegressionClassifier]\n",
        "\n",
        "trainFeatures.close()\n",
        "trainLabels.close()\n",
        "testFeatures.close()\n",
        "testLabels.close()\n",
        "trainSentences.close()\n",
        "testSentences.close()\n",
        "\n",
        "for ind, subset in enumerate(featureSubsets):\n",
        "    print(featureSubsetDetails[ind])\n",
        "    trainX = [[row[i] for i in subset] for row in fullTrainX]\n",
        "    testX = [[row[i] for i in subset] for row in fullTestX]\n",
        "    classifiers = [trainLogisticRegressionClassifier]#[trainRandomForestClassifier, trainCustomEnsemble, trainGradientBoostingClassifier, trainMLPClassifier]\n",
        "    outputs = []\n",
        "    models = []\n",
        "    plt.xlabel('Fraction Above Threshold') \n",
        "    plt.ylabel('Corpus BLEU score') \n",
        "    plt.title('Comparing Methods using Corpus BLEU score')\n",
        "    for j, classifier in enumerate(classifiers):\n",
        "        currFeaturesTrain = [[row[i] for i in featureSubsets[ind]] for row in featuresTrain]\n",
        "        currFeaturesTest = [[row[i] for i in featureSubsets[ind]] for row in featuresTest]\n",
        "        trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "        testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "        trainTranslations = readTranslations(trainSentences, currFeaturesTrain)\n",
        "        testTranslations = readTranslations(testSentences, currFeaturesTest)\n",
        "\n",
        "        print(\"#################################################\")\n",
        "        trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, 15, 15, False)\n",
        "        clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "        print(\"TRAIN ACCURACY\")\n",
        "        predictions = np.array(clf.predict(trainFeatures))\n",
        "        calculateAccuracy(predictions, trainY)\n",
        "        print(\"Percent acepted = \" + str(100 * calculatedAcceptedFraction(predictions)))\n",
        "        acceptedTranslations = np.array(trainTranslations)[np.array(predictions) > 0]\n",
        "        rejectedTranslations = np.array(trainTranslations)[np.array(predictions) < 1]\n",
        "        _, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "        print(\"Corpus BLEU score of accepted translations = \" + str(acceptedScore))\n",
        "\n",
        "        print(\"TEST ACCURACY\")\n",
        "        predictions = np.array(clf.predict(testFeatures))\n",
        "        calculateAccuracy(predictions, testY)\n",
        "        print(\"Percent acepted = \" + str(100 * calculatedAcceptedFraction(predictions)))\n",
        "        acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "        rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "        _, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "        print(\"Corpus BLEU score of accepted translations = \" + str(acceptedScore))\n",
        "\n",
        "        outputs.append(predictions)\n",
        "        models.append(clf)\n",
        "        print(\"#################################################\")\n",
        "\n",
        "        acceptedScores = []\n",
        "        acceptedFraction = []\n",
        "        print(\"TRAIN SET\")\n",
        "        for index in tqdm(range(len(testThresholds[j]))):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[j][index], testThresholds[j][index], avgLogProb[j])\n",
        "            # print(len(trainY))\n",
        "            clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(trainFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(trainTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(trainTranslations)[np.array(predictions) < 1]\n",
        "                \n",
        "            rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "            # print(\"Current threshold = \" + str(testThresholds[j][index]) + \" Current accepted Fraction = \" + str(float(len(acceptedTranslations))/float(len(predictions))))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[j]+\" train set\", color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "        print(\"[\"+modelLabel[j]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "\n",
        "        acceptedScores = []\n",
        "        acceptedFraction = []\n",
        "        print(\"TEST SET\")\n",
        "        for index in tqdm(range(len(testThresholds[j]))):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[j][index], testThresholds[j][index], avgLogProb[j])\n",
        "            # print(len(trainY))\n",
        "            clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(testFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "                \n",
        "            rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[j]+\" test set\", color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "\n",
        "        print(\"[\"+modelLabel[j]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2559, 22)\n",
            "(2559, 22)\n",
            "(2835, 22)\n",
            "TRAIN SET CLASS PROPORTIONS:\n",
            "Proportion in class 0.0 = 0.8014849550605705\n",
            "Proportion in class 1.0 = 0.19851504493942945\n",
            "TEST SET CLASS PROPORTIONS\n",
            "Proportion in class 1.0 = 0.2536155202821869\n",
            "Proportion in class 0.0 = 0.7463844797178131\n",
            "\n",
            "Just Forward Model score P(y|x)\n",
            "#################################################\n",
            "TRAIN ACCURACY\n",
            "Correctly accepted = 0.07874015748031496\n",
            "Incorrectly rejected = 0.9212598425196851\n",
            "Correctly rejected = 0.9907362262311068\n",
            "Incorrectly accepted = 0.009263773768893202\n",
            "Total Accuracy = 0.8096912856584604\n",
            "Percent acepted = 2.3055881203595154\n",
            "Corpus BLEU score of accepted translations = 22.55\n",
            "TEST ACCURACY\n",
            "Correctly accepted = 0.09040333796940195\n",
            "Incorrectly rejected = 0.9095966620305981\n",
            "Correctly rejected = 0.9891304347826086\n",
            "Incorrectly accepted = 0.010869565217391353\n",
            "Total Accuracy = 0.7611992945326279\n",
            "Percent acepted = 3.1040564373897706\n",
            "Corpus BLEU score of accepted translations = 24.36\n",
            "#################################################\n",
            "TRAIN SET\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6752a23b08d4e41ad2eee4b7d7fd6e2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccsFgQUiD656",
        "colab_type": "code",
        "outputId": "f656b135-539f-468f-a5e3-2ac75f401cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "\n",
        "dataset = \"test\"\n",
        "bleuThreshold = 15\n",
        "featureFile = open(\"ClassificationDataset/\"+str(bleuThreshold)+\"BLEU/\"+dataset+\"/features.data\")\n",
        "labelFile = open(\"ClassificationDataset/\"+str(bleuThreshold)+\"BLEU/\"+dataset+\"/result.data\")\n",
        "featuresUsed = [0]\n",
        "\n",
        "inputs, labels = datasetReader(featureFile, labelFile)\n",
        "for classifier in classifiers:\n",
        "    features = np.array([[row[i] for i in featuresUsed] for row in inputs])\n",
        "    kf = KFold(n_splits=len(features))\n",
        "\n",
        "    numCorrectTest = 0\n",
        "    currIter = 0\n",
        "    numCorrectTrain = 0\n",
        "    for train_index, test_index in kf.split(features):\n",
        "        trainX, trainY = features[train_index], labels[train_index]\n",
        "        testX, testY = features[test_index], labels[test_index]\n",
        "        if currIter == 0:\n",
        "            curr = classifier(trainX, trainY, verbose=True)\n",
        "        else:\n",
        "            curr = classifier(trainX, trainY, verbose=False)\n",
        "        \n",
        "        predictions = np.array(curr.predict(trainX))\n",
        "        for index, prediction in enumerate(predictions):\n",
        "            if prediction == trainY[index]:\n",
        "                numCorrectTrain += 1\n",
        "        \n",
        "        prediction = np.array(curr.predict(testX))\n",
        "\n",
        "        if prediction[0] == testY[0]:\n",
        "            numCorrectTest += 1\n",
        "        \n",
        "        currIter += 1\n",
        "        #   print(\"Current Accuracy = \" + str(float(numCorrect)/float(currIter)))\n",
        "    trainTotal = len(features) * (len(features) - 1)\n",
        "    print(\"Total Train Accuracy = \" + str(numCorrectTrain/trainTotal))\n",
        "    print(\"Total Test Accuracy = \" + str(numCorrectTest/len(features)))\n",
        "\n",
        "featureFile.close()\n",
        "labelFile.close()\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traning Logistic Regression Classifier\n",
            "Total Train Accuracy = 0.7682539682539683\n",
            "Total Test Accuracy = 0.7682539682539683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1uY1tRPfEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##make precision graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.data\", \"w\")\n",
        "    temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.data\", \"w\")\n",
        "\n",
        "    temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.data\", \"w\")\n",
        "    temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.data\", \"w\")\n",
        "    \n",
        "    for translation in acceptedTranslations:\n",
        "        temporary_reference_inclusion.write(translation.reference)\n",
        "        temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "    for translation in rejectedTranslations:\n",
        "        temporary_reference_exclusion.write(translation.reference)\n",
        "        temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "    \n",
        "    temporary_reference_inclusion.close()\n",
        "    temporary_output_inclusion.close()\n",
        "    temporary_reference_exclusion.close()\n",
        "    temporary_output_exclusion.close()\n",
        "\n",
        "    !fairseq-score --sys analysis/temporary_output_inclusion.data --ref analysis/temporary_reference_inclusion.data --sacrebleu > analysis/inclusion_result.data\n",
        "    !fairseq-score --sys analysis/temporary_output_exclusion.data --ref analysis/temporary_reference_exclusion.data --sacrebleu > analysis/exclusion_result.data\n",
        "\n",
        "    temporary_inclusion_result = open(\"analysis/inclusion_result.data\")\n",
        "    temporary_exclusion_result = open(\"analysis/exclusion_result.data\")\n",
        "    inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "    exclusion_result_string = [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "# featuresUsed = [0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "# featuresUsed = [0]\n",
        "# featuresUsed = [0, 4]\n",
        "\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.data\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.data\")\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.data\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.data\")\n",
        "\n",
        "trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "\n",
        "\n",
        "featuresTrain, _ = datasetReader(trainFeatures, trainLabels)\n",
        "featuresTest, _ = datasetReader(testFeatures, testLabels)\n",
        "\n",
        "featuresTrain = [[row[i] for i in featuresUsed] for row in featuresTrain]\n",
        "featuresTest = [[row[i] for i in featuresUsed] for row in featuresTest]\n",
        "\n",
        "trainTranslations = readTranslations(trainSentences, featuresTrain)\n",
        "testTranslations = readTranslations(testSentences, featuresTest)\n",
        "\n",
        "# Thresholds_train = np.linspace(-1.5, 0, 25).tolist()\n",
        "Thresholds_train = np.linspace(4, 28, 25).tolist()\n",
        "\n",
        "Thresholds_test = np.linspace(4, 28, 25).tolist()\n",
        "\n",
        "acceptedScores = []\n",
        "acceptedFraction = []\n",
        "\n",
        "useSentenceBLEUScore = True\n",
        "\n",
        "\n",
        "for index in range(len(Thresholds_test)):\n",
        "    trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, Thresholds_train[index], Thresholds_test[index])\n",
        "\n",
        "    clf = trainSVM(trainFeatures, trainY)\n",
        "    # print(\"Using Average Logprob Decision Stump of \" + str(Thresholds_train[index]))\n",
        "    # print(\"BLEU score = \" + str(Thresholds_test[index]))\n",
        "    predictions = clf.predict(testFeatures)\n",
        "    # calculateAccuracy(predictions, testY)\n",
        "    # print(\"##########################################\")\n",
        "    acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "    rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "    if useSentenceBLEUScore:\n",
        "        rejectedScore, acceptedScore = compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations)\n",
        "    else:\n",
        "        rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "    acceptedScores.append(acceptedScore)\n",
        "    acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "\n",
        "plt.xlabel('Fraction Above Threshold') \n",
        "plt.ylabel('BLEU score (average)') \n",
        "plt.title('Random Forest Thresholding') \n",
        "\n",
        "r = random.random()\n",
        "b = random.random()\n",
        "g = random.random()\n",
        "c = (r, g, b)\n",
        "plt.scatter(acceptedFraction, acceptedScores, label = \"Random Forest Analysis\", color=c)\n",
        "\n",
        "acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "acceptedFraction.sort()\n",
        "\n",
        "print('AUC for incuded fraction: {}'.format(auc(acceptedFraction, acceptedScores)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utOMembzJXAc",
        "colab_type": "text"
      },
      "source": [
        "Experimenting with multiple parameters and plotting curves on the same graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJFjlR8sJtkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    if len(acceptedTranslations) != 0:\n",
        "        temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.data\", \"w\")\n",
        "        temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.data\", \"w\")\n",
        "\n",
        "    \n",
        "        for translation in acceptedTranslations:\n",
        "            temporary_reference_inclusion.write(translation.reference)\n",
        "            temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_inclusion.data --ref analysis/temporary_reference_inclusion.data --sacrebleu > analysis/inclusion_result.data\n",
        "\n",
        "        temporary_inclusion_result = open(\"analysis/inclusion_result.data\")\n",
        "        inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    else:\n",
        "        inclusion_result_string = \"0\"\n",
        "\n",
        "    if len(rejectedTranslations) != 0:\n",
        "\n",
        "        temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.data\", \"w\")\n",
        "        temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.data\", \"w\")\n",
        "        \n",
        "        for translation in rejectedTranslations:\n",
        "            temporary_reference_exclusion.write(translation.reference)\n",
        "            temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "        \n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_exclusion.data --ref analysis/temporary_reference_exclusion.data --sacrebleu > analysis/exclusion_result.data\n",
        "\n",
        "        temporary_exclusion_result = open(\"analysis/exclusion_result.data\")\n",
        "        exclusion_result_string = \"0\" if len(rejectedTranslations) == 0 else [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "    \n",
        "    else:\n",
        "        exclusion_result_string = \"0\"\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.data\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.data\")\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.data\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.data\")\n",
        "\n",
        "\n",
        "featuresTrain, _ = datasetReader(trainFeatures, trainLabels)\n",
        "featuresTest, _ = datasetReader(testFeatures, testLabels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ItLAR-5J-ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featuresUsed = [[0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [0], [4]]\n",
        "trainThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(-1.5, -0.25, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "testThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(4, 28, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "\n",
        "evaluationMetrics = [compute_exclued_included_sentenceBleuScore, compute_excluded_included_score]\n",
        "evalLabel = {0: \"Average Sentence BLEU score\", 1: \"Corpus BLEU score\"}\n",
        "models = [trainRandomForestClassifier, trainRandomForestClassifier, trainRandomForestClassifier]\n",
        "modelLabel = {0: \"Random Forest Classifier (all features)\", 1: \"Average Logprob Thresholding\", 2: \"Sentence BLEU Thresholding\"}\n",
        "avgLogProb = [False, True, False]\n",
        "acceptedScores = []\n",
        "acceptedFraction = []\n",
        "\n",
        "\n",
        "for evalMetric in range(len(evaluationMetrics)):\n",
        "    plt.xlabel('Fraction Above Threshold') \n",
        "    plt.ylabel(evalLabel[evalMetric]) \n",
        "    plt.title('Comparing Methods using ' + evalLabel[evalMetric])\n",
        "\n",
        "    for model in range(len(models)):\n",
        "        currFeaturesTrain = [[row[i] for i in featuresUsed[model]] for row in featuresTrain]\n",
        "        currFeaturesTest = [[row[i] for i in featuresUsed[model]] for row in featuresTest]\n",
        "\n",
        "\n",
        "        trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.data\")\n",
        "        testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.data\")\n",
        "\n",
        "        trainTranslations = readTranslations(trainSentences, currFeaturesTrain)\n",
        "        testTranslations = readTranslations(testSentences, currFeaturesTest)\n",
        "\n",
        "        acceptedScores = []\n",
        "        acceptedFraction = []\n",
        "\n",
        "        for index in range(len(testThresholds[model])):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[model][index], testThresholds[model][index], avgLogProb[model])\n",
        "            clf = models[model](trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(testFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "              \n",
        "            rejectedScore, acceptedScore = evaluationMetrics[evalMetric](acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[model], color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "\n",
        "        print(\"[\"+modelLabel[model]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}