{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureSubsetBinaryClassifers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhruvaBansal00/ConfidentMT/blob/master/FeatureSubsetBinaryClassifers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEk99K4f0Hw_",
        "colab_type": "code",
        "outputId": "838625bc-cc9d-47f9-a3fe-f819f495fd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls\n",
        "%cd drive/My Drive/ConfidentMachineTranslation/flores\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n",
            "/content/drive/.shortcut-targets-by-id/1viBwZM7BIiD8O4LeU-9UQXyV82oTWqh6/ConfidentMachineTranslation/flores\n",
            "'=0.5.0'\t\t\t FeatureSubsetBinaryClassifers.ipynb\n",
            " analysis\t\t\t FLORES.ipynb\n",
            " Analysis\t\t\t LanguageAnalysis.ipynb\n",
            " backward_models\t\t language_models\n",
            " BoostedBinaryClassifers.ipynb\t LM_Thresholding.ipynb\n",
            " checkpoints\t\t\t NCD_Analysis.ipynb\n",
            " ClassificationDataset\t\t NNClassification.ipynb\n",
            " configs\t\t\t noisychannel\n",
            " data\t\t\t\t NoisyChannel.ipynb\n",
            " data-bin\t\t\t Resources\n",
            " Ensembles\t\t\t scripts\n",
            " Ensembling\n",
            "/content/drive/.shortcut-targets-by-id/1viBwZM7BIiD8O4LeU-9UQXyV82oTWqh6/ConfidentMachineTranslation/flores\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0LVqC0npSH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "33c00f11-10d0-4464-9d9b-eb664763523c"
      },
      "source": [
        "pip install fairseq sacrebleu sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 2.8MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021146 sha256=1ca91ccfc77ebec8951b550edb348265d9b29a2e2e68aabcfdf361e404606691\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq, sentencepiece\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.9 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlcgFl162kdH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e1e2ed13-3a94-483f-d339-378b149624fa"
      },
      "source": [
        "# prints how much GPU RAM is available\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=7adfac5c19ac21ab7a591250457d54fcc699fb6523e01f23aa71b9619a8fa07c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 159.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xwd7lNk3EZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import numpy as np\n",
        "from itertools import zip_longest\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "class CustomEnsembleClassifier:\n",
        "    def __init__(self, clfs):\n",
        "        self.classifiers = clfs\n",
        "    \n",
        "    def predict(self, X):\n",
        "        probabilities = None\n",
        "        for clf in self.classifiers:\n",
        "            if probabilities is None:\n",
        "                probabilities = clf.predict_proba(X)\n",
        "            else:\n",
        "                probabilities += clf.predict_proba(X)\n",
        "        return np.argmax(np.array(probabilities), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "def printDatasetClassProp(Y): \n",
        "    classes = {}\n",
        "    total = len(Y)\n",
        "    for i in Y:\n",
        "        if i in classes:\n",
        "            classes[i] += 1\n",
        "        else:\n",
        "            classes[i] = 1\n",
        "    \n",
        "    for cls in classes:\n",
        "        print(\"Proportion in class \" + str(cls) + \" = \" + str(classes[cls]/total))\n",
        "\n",
        "def datasetReader(featureFile, labelFile):\n",
        "    files = [featureFile, labelFile]\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for lines in zip_longest(*files, fillvalue=''):\n",
        "        currX, currY = lines[0], float(lines[1].strip(\"\\n\"))\n",
        "        Xarr = []\n",
        "        features = currX.split()\n",
        "        for feature in features:\n",
        "            Xarr.append(float(feature.strip(\",\").strip(\"\\n\")))\n",
        "        X.append(Xarr)\n",
        "        Y.append(currY)    \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def computeSimilarity(o1, o2):\n",
        "    total = len(o1)\n",
        "    same = 0\n",
        "    for i in range(len(o1)):\n",
        "        if o1[i] == o2[i]:\n",
        "            same += 1\n",
        "    print(same/total)\n",
        "\n",
        "\n",
        "def trainMLPClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training MLP Classifier\")\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(64, 256, 512, 256, 64), random_state=42,\n",
        "                        max_iter=200, learning_rate='adaptive', learning_rate_init=0.0005, activation='relu')\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainKNeighborsClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training KNeighbors Classifier\")\n",
        "    clf = KNeighborsClassifier(100)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainGaussianProcessClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Gaussian Process Classifier\")\n",
        "    length_scale = [1 for i in range(len(X[0]))]\n",
        "    clf = GaussianProcessClassifier(1.0 * RBF(length_scale), warm_start=True, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainCustomEnsemble(X, Y, maxDepth=8, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training custom ensemble\")\n",
        "    rf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    grad = GradientBoostingClassifier(random_state=42)\n",
        "    ada = AdaBoostClassifier(n_estimators=estimators, random_state=42)\n",
        "    # dl = MLPClassifier(hidden_layer_sizes=(100), random_state=1, max_iter=200)\n",
        "    # kn = KNeighborsClassifier(100)\n",
        "\n",
        "    classifiers = [rf, grad, ada]\n",
        "\n",
        "    for clf in classifiers:\n",
        "        clf.fit(X, Y)\n",
        "\n",
        "    return CustomEnsembleClassifier(classifiers)\n",
        "    \n",
        "\n",
        "def trainEnsembleClassifier(X, Y, maxDepth=8, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training an ensemble of Random Forest and Gradient Boosting Classifiers\")\n",
        "\n",
        "    estimators = [\n",
        "     ('rf', RandomForestClassifier(max_depth=maxDepth, random_state=42)),\n",
        "     ('grad', GradientBoostingClassifier(random_state=42))]\n",
        "    clf = StackingClassifier(estimators=estimators, final_estimator=AdaBoostClassifier(n_estimators=50, random_state=42))\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def trainRandomForestClassifier(X, Y, maxDepth=8, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Random Forest classifier\")\n",
        "    clf = RandomForestClassifier(max_depth=maxDepth, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainAdaBoostClassifier(X, Y, estimators=100, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training AdaBoosted Decision Tree classifier\")\n",
        "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=estimators, random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainGradientBoostingClassifier(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training Graident Boosted classifier\")\n",
        "    clf = GradientBoostingClassifier(random_state=42)\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def trainSVM(X, Y, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Training SVM classifier\")\n",
        "    clf = SVC(gamma='auto')\n",
        "    clf.fit(X, Y)\n",
        "    return clf\n",
        "\n",
        "def calculateAccuracy(predictedClasses, groundTruth):\n",
        "    correct_accepted = 0\n",
        "    total_accepted = 0\n",
        "\n",
        "    correct_rejected = 0\n",
        "    total_rejected = 0\n",
        "\n",
        "    for i in range(len(predictedClasses)):\n",
        "        if groundTruth[i] == 1:\n",
        "            total_accepted += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_accepted += 1\n",
        "        else:\n",
        "            total_rejected += 1\n",
        "            if predictedClasses[i] == groundTruth[i]:\n",
        "                correct_rejected += 1\n",
        "\n",
        "\n",
        "    print(\"Correctly accepted = \" + str(correct_accepted/total_accepted))\n",
        "    print(\"Incorrectly rejected = \" + str(1 - correct_accepted/total_accepted))\n",
        "    print(\"Correctly rejected = \" + str(correct_rejected/total_rejected))\n",
        "    print(\"Incorrectly accepted = \" + str(1 - correct_rejected/total_rejected))\n",
        "\n",
        "    print(\"Total Accuracy = \" + str((correct_accepted + correct_rejected)/(total_accepted + total_rejected)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_4LpbZ6UBsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b61b09a-c471-418a-fe31-f2e688e55902"
      },
      "source": [
        "##make precision graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.txt\", \"w\")\n",
        "    temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.txt\", \"w\")\n",
        "\n",
        "    temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.txt\", \"w\")\n",
        "    temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.txt\", \"w\")\n",
        "    \n",
        "    for translation in acceptedTranslations:\n",
        "        temporary_reference_inclusion.write(translation.reference)\n",
        "        temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "    for translation in rejectedTranslations:\n",
        "        temporary_reference_exclusion.write(translation.reference)\n",
        "        temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "    \n",
        "    temporary_reference_inclusion.close()\n",
        "    temporary_output_inclusion.close()\n",
        "    temporary_reference_exclusion.close()\n",
        "    temporary_output_exclusion.close()\n",
        "\n",
        "    !fairseq-score --sys analysis/temporary_output_inclusion.txt --ref analysis/temporary_reference_inclusion.txt --sacrebleu > analysis/inclusion_result.txt\n",
        "    !fairseq-score --sys analysis/temporary_output_exclusion.txt --ref analysis/temporary_reference_exclusion.txt --sacrebleu > analysis/exclusion_result.txt\n",
        "\n",
        "    temporary_inclusion_result = open(\"analysis/inclusion_result.txt\")\n",
        "    temporary_exclusion_result = open(\"analysis/exclusion_result.txt\")\n",
        "    if (len([line for line in temporary_inclusion_result]) > 1 and len([line for line in temporary_inclusion_result][1].split(\" \")[2] > 2)):\n",
        "      inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "    if (len([line for line in temporary_exclusion_result]) > 1 and len([line for line in temporary_exclusion_result][1].split(\" \")[2] > 2)):\n",
        "      exclusion_result_string = [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "# featuresUsed = [0, 5, 6, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20]\n",
        "# featuresUsed = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] ##All\n",
        "avgLogProb = [False, False, False, False]\n",
        "featuresUsed = [0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "# featuresUsed = [10, 11, 12]\n",
        "featureSubsets = [[10], #just baseline Forward Model score [P(T|S)]\n",
        "                  [10, 11, 12], #Similar to noisy channel decoding: P(y|x), p(x|y), p(y) \n",
        "                  [10, 11, 12, 19, 20], #Like above + sentence length features\n",
        "                  [0, 10, 11, 12, 19, 20], #Like above + average Logprob + sentence length features\n",
        "                  [0, 5, 6, 10, 11, 12, 19, 20], #Like above + average Logprob + sentence length features + Rare words\n",
        "                  [0, 10, 11, 12, 13, 14, 19, 20], #Like above + average Logprob + sentence length features + end of sentence identifiers\n",
        "                  [10, 11, 12, 16, 17, 18], #Like above + ngram features\n",
        "                  [0, 10, 11, 12, 16, 17, 18], #Like above + average Logprob + ngram features\n",
        "                  [0, 5, 6, 10, 11, 12, 13, 14, 19, 20] #Like above + average Logprob + sentence length features + Rare words + end of sentence identifiers\n",
        "                  ]\n",
        "featureSubsetDetails = [\"Just baseline Forward Model score [P(T|S)]\",\n",
        "                        \"Similar to noisy channel decoding: P(y|x), p(x|y), p(y)\",\n",
        "                        \"NCD features + sentence length features\",\n",
        "                        \"NCD features + average Logprob + sentence length features\",\n",
        "                        \"NCD features + average Logprob + sentence length features + Rare words\",\n",
        "                        \"NCD features + average Logprob + sentence length features + end of sentence identifiers\",\n",
        "                        \"NCD features + ngram features\",\n",
        "                        \"NCD features + average Logprob + ngram features\",\n",
        "                        \"NCD features + average Logprob + sentence length features + Rare words + end of sentence identifiers\"]\n",
        "\n",
        "trainThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(-1.5, -0.25, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "testThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(4, 28, 25).tolist(), np.linspace(1, 60, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "modelLabel = {0: \"Random Forest Classifier\", 1: \"Custom Ensemble\", 2: \"Gradient Boosting Classifier\", 3: \"MLP Classifier\"}\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.txt\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.txt\")\n",
        "\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.txt\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.txt\")\n",
        "\n",
        "trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.txt\")\n",
        "testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.txt\")\n",
        "fullTrainX, fullTrainY = datasetReader(trainFeatures, trainLabels)\n",
        "fullTestX, fullTestY = datasetReader(testFeatures, testLabels)\n",
        "featuresTrain = fullTrainX\n",
        "featuresTest = fullTestX\n",
        "print(featuresTrain.shape)\n",
        "# print(len(trainX[0]))\n",
        "# print(len(testX[0]))\n",
        "\n",
        "print(np.array(fullTrainX).shape)\n",
        "print(np.array(fullTestX).shape)\n",
        "print(\"TRAIN SET CLASS PROPORTIONS:\")\n",
        "printDatasetClassProp(fullTrainY)\n",
        "print(\"TEST SET CLASS PROPORTIONS\")\n",
        "printDatasetClassProp(fullTestY)\n",
        "print()\n",
        "for ind, subset in enumerate(featureSubsets):\n",
        "  trainX = [[row[i] for i in subset] for row in fullTrainX]\n",
        "  testX = [[row[i] for i in subset] for row in fullTestX]\n",
        "\n",
        "  print(np.array(trainX).shape)\n",
        "  print(np.array(testX).shape)\n",
        "  classifiers = [trainRandomForestClassifier, trainCustomEnsemble, trainGradientBoostingClassifier, trainMLPClassifier]\n",
        "  outputs = []\n",
        "  models = []\n",
        "  plt.xlabel('Fraction Above Threshold') \n",
        "  plt.ylabel('Corpus BLEU score') \n",
        "  plt.title('Comparing Methods using Corpus BLEU score')\n",
        "  for j, classifier in enumerate(classifiers):\n",
        "      print(\"#################################################\")\n",
        "      curr = classifier(trainX, fullTrainY)\n",
        "      print(\"TRAIN ACCURACY\")\n",
        "      predictions = np.array(curr.predict(trainX))\n",
        "      calculateAccuracy(predictions, fullTrainY)\n",
        "      print(\"TEST ACCURACY\")\n",
        "      predictions = np.array(curr.predict(testX))\n",
        "      calculateAccuracy(predictions, fullTestY)\n",
        "      outputs.append(predictions)\n",
        "      models.append(curr)\n",
        "      print(\"#################################################\")\n",
        "      currFeaturesTrain = [[row[i] for i in featureSubsets[ind]] for row in featuresTrain]\n",
        "      currFeaturesTest = [[row[i] for i in featureSubsets[ind]] for row in featuresTest]\n",
        "      trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.txt\")\n",
        "      testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.txt\")\n",
        "\n",
        "      trainTranslations = readTranslations(trainSentences, currFeaturesTrain)\n",
        "      testTranslations = readTranslations(testSentences, currFeaturesTest)\n",
        "\n",
        "      acceptedScores = []\n",
        "      acceptedFraction = []\n",
        "      if j != 1:\n",
        "        print(\"TRAIN SET\")\n",
        "        for index in range(len(testThresholds[j])):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[j][index], testThresholds[j][index], avgLogProb[j])\n",
        "            print(len(trainY))\n",
        "            clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(trainFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(trainTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(trainTranslations)[np.array(predictions) < 1]\n",
        "              \n",
        "            rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[j], color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "\n",
        "        print(\"[\"+modelLabel[j]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "        print(\"TEST SET\")\n",
        "        for index in range(len(testThresholds[j])):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[j][index], testThresholds[j][index], avgLogProb[j])\n",
        "            print(len(trainY))\n",
        "            clf = classifier(trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(testFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "              \n",
        "            rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[j], color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "\n",
        "        print(\"[\"+modelLabel[j]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "  print(\"\\n\\n\")\n",
        "  \n",
        "'''for output_1 in outputs:\n",
        "    for output_2 in outputs:\n",
        "        computeSimilarity(output_1, output_2)'''\n",
        "\n",
        "\n",
        "trainFeatures.close()\n",
        "trainLabels.close()\n",
        "testFeatures.close()\n",
        "testLabels.close()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2559, 23)\n",
            "(2559, 23)\n",
            "(2835, 23)\n",
            "TRAIN SET CLASS PROPORTIONS:\n",
            "Proportion in class 0.0 = 0.8014849550605705\n",
            "Proportion in class 1.0 = 0.19851504493942945\n",
            "TEST SET CLASS PROPORTIONS\n",
            "Proportion in class 1.0 = 0.2536155202821869\n",
            "Proportion in class 0.0 = 0.7463844797178131\n",
            "\n",
            "(2559, 1)\n",
            "(2835, 1)\n",
            "#################################################\n",
            "Training Random Forest classifier\n",
            "TRAIN ACCURACY\n",
            "Correctly accepted = 0.34448818897637795\n",
            "Incorrectly rejected = 0.655511811023622\n",
            "Correctly rejected = 0.98488542174549\n",
            "Incorrectly accepted = 0.015114578254510014\n",
            "Total Accuracy = 0.8577569363032435\n",
            "TEST ACCURACY\n",
            "Correctly accepted = 0.2141863699582754\n",
            "Incorrectly rejected = 0.7858136300417247\n",
            "Correctly rejected = 0.9428166351606805\n",
            "Incorrectly accepted = 0.05718336483931952\n",
            "Total Accuracy = 0.7580246913580246\n",
            "#################################################\n",
            "TRAIN SET\n",
            "2559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5537e6abc8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mrejectedTranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainTranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mrejectedScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macceptedScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_excluded_included_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macceptedTranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejectedTranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0macceptedScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macceptedScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-5537e6abc8fa>\u001b[0m in \u001b[0;36mcompute_excluded_included_score\u001b[0;34m(acceptedTranslations, rejectedTranslations)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtemporary_inclusion_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"analysis/inclusion_result.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtemporary_exclusion_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"analysis/exclusion_result.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemporary_inclusion_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemporary_inclusion_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m       \u001b[0minclusion_result_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemporary_inclusion_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemporary_exclusion_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemporary_exclusion_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccsFgQUiD656",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "\n",
        "dataset = \"valid\"\n",
        "bleuThreshold = 15\n",
        "featureFile = open(\"ClassificationDataset/\"+str(bleuThreshold)+\"BLEU/\"+dataset+\"/features.txt\")\n",
        "labelFile = open(\"ClassificationDataset/\"+str(bleuThreshold)+\"BLEU/\"+dataset+\"/result.txt\")\n",
        "featuresUsed = [10]\n",
        "\n",
        "inputs, labels = datasetReader(featureFile, labelFile)\n",
        "for classifier in classifiers:\n",
        "  features = np.array([[row[i] for i in featuresUsed] for row in inputs])\n",
        "  kf = KFold(n_splits=len(features))\n",
        "\n",
        "  numCorrect = 0\n",
        "  currIter = 0\n",
        "  for train_index, test_index in kf.split(features):\n",
        "      print(\"Currently done with \" + str(currIter)+\"/\"+str(len(features)))\n",
        "      trainX, trainY = features[train_index], labels[train_index]\n",
        "      testX, testY = features[test_index], labels[test_index]\n",
        "      if currIter == 0:\n",
        "        curr = classifier(trainX, trainY, verbose=True)\n",
        "      else:\n",
        "        curr = classifier(trainX, trainY, verbose=False)\n",
        "      prediction = np.array(curr.predict(testX))\n",
        "\n",
        "      if prediction[0] == testY[0]:\n",
        "          numCorrect += 1\n",
        "      \n",
        "      currIter += 1\n",
        "      print(\"Current Accuracy = \" + str(float(numCorrect)/float(currIter)))\n",
        "\n",
        "  print(\"Total Accuracy = \" + str(numCorrect/len(features)))\n",
        "\n",
        "featureFile.close()\n",
        "labelFile.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1uY1tRPfEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "ded1df09-8b80-4bd9-c435-758aae17b4d7"
      },
      "source": [
        "##make precision graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.txt\", \"w\")\n",
        "    temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.txt\", \"w\")\n",
        "\n",
        "    temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.txt\", \"w\")\n",
        "    temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.txt\", \"w\")\n",
        "    \n",
        "    for translation in acceptedTranslations:\n",
        "        temporary_reference_inclusion.write(translation.reference)\n",
        "        temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "    for translation in rejectedTranslations:\n",
        "        temporary_reference_exclusion.write(translation.reference)\n",
        "        temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "    \n",
        "    temporary_reference_inclusion.close()\n",
        "    temporary_output_inclusion.close()\n",
        "    temporary_reference_exclusion.close()\n",
        "    temporary_output_exclusion.close()\n",
        "\n",
        "    !fairseq-score --sys analysis/temporary_output_inclusion.txt --ref analysis/temporary_reference_inclusion.txt --sacrebleu > analysis/inclusion_result.txt\n",
        "    !fairseq-score --sys analysis/temporary_output_exclusion.txt --ref analysis/temporary_reference_exclusion.txt --sacrebleu > analysis/exclusion_result.txt\n",
        "\n",
        "    temporary_inclusion_result = open(\"analysis/inclusion_result.txt\")\n",
        "    temporary_exclusion_result = open(\"analysis/exclusion_result.txt\")\n",
        "    inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "    exclusion_result_string = [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "# featuresUsed = [0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
        "# featuresUsed = [0]\n",
        "# featuresUsed = [0, 4]\n",
        "\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 15\n",
        "bleuThresholdTest = 15\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.txt\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.txt\")\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.txt\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.txt\")\n",
        "\n",
        "trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.txt\")\n",
        "testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.txt\")\n",
        "\n",
        "\n",
        "featuresTrain, _ = datasetReader(trainFeatures, trainLabels)\n",
        "featuresTest, _ = datasetReader(testFeatures, testLabels)\n",
        "\n",
        "featuresTrain = [[row[i] for i in featuresUsed] for row in featuresTrain]\n",
        "featuresTest = [[row[i] for i in featuresUsed] for row in featuresTest]\n",
        "\n",
        "trainTranslations = readTranslations(trainSentences, featuresTrain)\n",
        "testTranslations = readTranslations(testSentences, featuresTest)\n",
        "\n",
        "# Thresholds_train = np.linspace(-1.5, 0, 25).tolist()\n",
        "Thresholds_train = np.linspace(4, 28, 25).tolist()\n",
        "\n",
        "Thresholds_test = np.linspace(4, 28, 25).tolist()\n",
        "\n",
        "acceptedScores = []\n",
        "acceptedFraction = []\n",
        "\n",
        "useSentenceBLEUScore = True\n",
        "\n",
        "\n",
        "for index in range(len(Thresholds_test)):\n",
        "    trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, Thresholds_train[index], Thresholds_test[index])\n",
        "\n",
        "    clf = trainSVM(trainFeatures, trainY)\n",
        "    # print(\"Using Average Logprob Decision Stump of \" + str(Thresholds_train[index]))\n",
        "    # print(\"BLEU score = \" + str(Thresholds_test[index]))\n",
        "    predictions = clf.predict(testFeatures)\n",
        "    # calculateAccuracy(predictions, testY)\n",
        "    # print(\"##########################################\")\n",
        "    acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "    rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "    if useSentenceBLEUScore:\n",
        "        rejectedScore, acceptedScore = compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations)\n",
        "    else:\n",
        "        rejectedScore, acceptedScore = compute_excluded_included_score(acceptedTranslations, rejectedTranslations)\n",
        "    acceptedScores.append(acceptedScore)\n",
        "    acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "\n",
        "plt.xlabel('Fraction Above Threshold') \n",
        "plt.ylabel('BLEU score (average)') \n",
        "plt.title('Random Forest Thresholding') \n",
        "\n",
        "r = random.random()\n",
        "b = random.random()\n",
        "g = random.random()\n",
        "c = (r, g, b)\n",
        "plt.scatter(acceptedFraction, acceptedScores, label = \"Random Forest Analysis\", color=c)\n",
        "\n",
        "acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "acceptedFraction.sort()\n",
        "\n",
        "print('AUC for incuded fraction: {}'.format(auc(acceptedFraction, acceptedScores)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c72493789813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mThresholds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mtrainFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainTestSets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainTranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestTranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThresholds_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThresholds_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getTrainTestSets() missing 1 required positional argument: 'avgLogProb'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utOMembzJXAc",
        "colab_type": "text"
      },
      "source": [
        "Experimenting with multiple parameters and plotting curves on the same graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJFjlR8sJtkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "import random\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, original, reference, translation, score, features):\n",
        "        self.original = original\n",
        "        self.reference = reference\n",
        "        self.translation = translation\n",
        "        self.score = score\n",
        "        self.features = features\n",
        "\n",
        "def compute_exclued_included_sentenceBleuScore(acceptedTranslations, rejectedTranslations):\n",
        "    acceptedScore = 0 if len(acceptedTranslations) == 0 else sum([translation.score for translation in acceptedTranslations])/len(acceptedTranslations)\n",
        "    \n",
        "    rejectedScore = 0 if len(rejectedTranslations) == 0 else sum([translation.score for translation in rejectedTranslations])/len(rejectedTranslations)\n",
        "\n",
        "    return rejectedScore, acceptedScore\n",
        "\n",
        "def compute_excluded_included_score (acceptedTranslations, rejectedTranslations):\n",
        "    if len(acceptedTranslations) != 0:\n",
        "        temporary_reference_inclusion = open(\"analysis/temporary_reference_inclusion.txt\", \"w\")\n",
        "        temporary_output_inclusion = open(\"analysis/temporary_output_inclusion.txt\", \"w\")\n",
        "\n",
        "    \n",
        "        for translation in acceptedTranslations:\n",
        "            temporary_reference_inclusion.write(translation.reference)\n",
        "            temporary_output_inclusion.write(translation.translation)\n",
        "\n",
        "        temporary_reference_inclusion.close()\n",
        "        temporary_output_inclusion.close()\n",
        "\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_inclusion.txt --ref analysis/temporary_reference_inclusion.txt --sacrebleu > analysis/inclusion_result.txt\n",
        "\n",
        "        temporary_inclusion_result = open(\"analysis/inclusion_result.txt\")\n",
        "        inclusion_result_string = [line for line in temporary_inclusion_result][1].split(\" \")[2]\n",
        "\n",
        "    else:\n",
        "        inclusion_result_string = \"0\"\n",
        "\n",
        "    if len(rejectedTranslations) != 0:\n",
        "\n",
        "        temporary_reference_exclusion = open(\"analysis/temporary_reference_exclusion.txt\", \"w\")\n",
        "        temporary_output_exclusion = open(\"analysis/temporary_output_exclusion.txt\", \"w\")\n",
        "        \n",
        "        for translation in rejectedTranslations:\n",
        "            temporary_reference_exclusion.write(translation.reference)\n",
        "            temporary_output_exclusion.write(translation.translation)\n",
        "\n",
        "        \n",
        "        temporary_reference_exclusion.close()\n",
        "        temporary_output_exclusion.close()\n",
        "\n",
        "        !fairseq-score --sys analysis/temporary_output_exclusion.txt --ref analysis/temporary_reference_exclusion.txt --sacrebleu > analysis/exclusion_result.txt\n",
        "\n",
        "        temporary_exclusion_result = open(\"analysis/exclusion_result.txt\")\n",
        "        exclusion_result_string = \"0\" if len(rejectedTranslations) == 0 else [line for line in temporary_exclusion_result][1].split(\" \")[2]\n",
        "    \n",
        "    else:\n",
        "        exclusion_result_string = \"0\"\n",
        "\n",
        "    return float(exclusion_result_string), float(inclusion_result_string)\n",
        "\n",
        "\n",
        "def readTranslations(sentenceFile, featureArray):\n",
        "    translations = []\n",
        "    temp = []\n",
        "    index = 0\n",
        "    for line in sentenceFile:\n",
        "        if len(temp) < 3:\n",
        "            temp.append(line)\n",
        "        else:\n",
        "            score = float(line.strip(\"\\n\"))\n",
        "            translations.append(Translation(temp[0], temp[1], temp[2], score, featureArray[index]))\n",
        "            index += 1\n",
        "            temp = []\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def getTrainTestSets(trainTranslations, testTranslations, threshold_train, threshold_test, avgLogProb):\n",
        "    trainFeatures = []\n",
        "    trainY = []\n",
        "    testFeatures = []\n",
        "    testY = []\n",
        "\n",
        "    for translation in trainTranslations:\n",
        "        trainFeatures.append(translation.features)\n",
        "        if avgLogProb:\n",
        "            if translation.features[0] < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "        else:\n",
        "            if translation.score < threshold_train:\n",
        "                trainY.append(0)\n",
        "            else:\n",
        "                trainY.append(1)\n",
        "    \n",
        "    for translation in testTranslations:\n",
        "        testFeatures.append(translation.features)\n",
        "        if translation.score < threshold_test:\n",
        "            testY.append(0)\n",
        "        else:\n",
        "            testY.append(1)\n",
        "\n",
        "    return trainFeatures, trainY, testFeatures, testY\n",
        "\n",
        "trainset = \"valid\"\n",
        "testset = \"test\"\n",
        "bleuThresholdTrain = 10\n",
        "bleuThresholdTest = 10\n",
        "\n",
        "trainFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/features.txt\")\n",
        "testFeatures = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/features.txt\")\n",
        "trainLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/result.txt\")\n",
        "testLabels = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/result.txt\")\n",
        "\n",
        "\n",
        "featuresTrain, _ = datasetReader(trainFeatures, trainLabels)\n",
        "featuresTest, _ = datasetReader(testFeatures, testLabels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ItLAR-5J-ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featuresUsed = [[0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [0], [4]]\n",
        "trainThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(-1.5, -0.25, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "testThresholds = [np.linspace(4, 28, 25).tolist(), np.linspace(4, 28, 25).tolist(), np.linspace(1, 60, 25).tolist()]\n",
        "\n",
        "evaluationMetrics = [compute_exclued_included_sentenceBleuScore, compute_excluded_included_score]\n",
        "evalLabel = {0: \"Average Sentence BLEU score\", 1: \"Corpus BLEU score\"}\n",
        "models = [trainRandomForestClassifier, trainRandomForestClassifier, trainRandomForestClassifier]\n",
        "modelLabel = {0: \"Random Forest Classifier (all features)\", 1: \"Average Logprob Thresholding\", 2: \"Sentence BLEU Thresholding\"}\n",
        "avgLogProb = [False, True, False]\n",
        "acceptedScores = []\n",
        "acceptedFraction = []\n",
        "\n",
        "\n",
        "for evalMetric in range(len(evaluationMetrics)):\n",
        "    plt.xlabel('Fraction Above Threshold') \n",
        "    plt.ylabel(evalLabel[evalMetric]) \n",
        "    plt.title('Comparing Methods using ' + evalLabel[evalMetric])\n",
        "\n",
        "    for model in range(len(models)):\n",
        "        currFeaturesTrain = [[row[i] for i in featuresUsed[model]] for row in featuresTrain]\n",
        "        currFeaturesTest = [[row[i] for i in featuresUsed[model]] for row in featuresTest]\n",
        "\n",
        "\n",
        "        trainSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTrain)+\"BLEU/\"+trainset+\"/sentences.txt\")\n",
        "        testSentences = open(\"ClassificationDataset/\"+str(bleuThresholdTest)+\"BLEU/\"+testset+\"/sentences.txt\")\n",
        "\n",
        "        trainTranslations = readTranslations(trainSentences, currFeaturesTrain)\n",
        "        testTranslations = readTranslations(testSentences, currFeaturesTest)\n",
        "\n",
        "        acceptedScores = []\n",
        "        acceptedFraction = []\n",
        "\n",
        "        for index in range(len(testThresholds[model])):\n",
        "            trainFeatures, trainY, testFeatures, testY = getTrainTestSets(trainTranslations, testTranslations, trainThresholds[model][index], testThresholds[model][index], avgLogProb[model])\n",
        "            clf = models[model](trainFeatures, trainY, verbose=False)\n",
        "            predictions = clf.predict(testFeatures)\n",
        "            \n",
        "            acceptedTranslations = np.array(testTranslations)[np.array(predictions) > 0]\n",
        "            rejectedTranslations = np.array(testTranslations)[np.array(predictions) < 1]\n",
        "              \n",
        "            rejectedScore, acceptedScore = evaluationMetrics[evalMetric](acceptedTranslations, rejectedTranslations)\n",
        "            \n",
        "            acceptedScores.append(acceptedScore)\n",
        "            acceptedFraction.append(float(len(acceptedTranslations))/float(len(predictions)))\n",
        "        \n",
        "        r = random.random()\n",
        "        b = random.random()\n",
        "        g = random.random()\n",
        "        c = (r, g, b)\n",
        "        plt.plot(acceptedFraction, acceptedScores, label = modelLabel[model], color=c)\n",
        "        acceptedScores = [x for _,x in sorted(zip(acceptedFraction,acceptedScores))]\n",
        "        acceptedFraction.sort()\n",
        "\n",
        "        print(\"[\"+modelLabel[model]+\"] AUC for included fraction: {}\".format(auc(acceptedFraction, acceptedScores)))\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}